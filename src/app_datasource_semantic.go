package main

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"path/filepath"
	"strings"
	"time"

	"vantagics/agent"
	"vantagics/i18n"
	
	"github.com/wailsapp/wails/v2/pkg/runtime"
)

// TableSchemaInfo è¡¨ç»“æ„ä¿¡æ?
type TableSchemaInfo struct {
	TableName  string              `json:"table_name"`
	Columns    []ColumnInfo        `json:"columns"`
	SampleData []map[string]any    `json:"sample_data"`
}

// ColumnInfo å­—æ®µä¿¡æ¯
type ColumnInfo struct {
	Name     string `json:"name"`
	Type     string `json:"type"`
	Nullable bool   `json:"nullable"`
}

// SemanticOptimizationResult LLM è¿”å›çš„ä¼˜åŒ–ç»“æ?
type SemanticOptimizationResult struct {
	Tables []TableOptimization `json:"tables"`
}

// TableOptimization è¡¨çš„ä¼˜åŒ–ç»“æœ
type TableOptimization struct {
	OriginalTableName  string          `json:"original_table_name"`
	OptimizedTableName string          `json:"optimized_table_name"`
	ColumnMappings     []ColumnMapping `json:"column_mappings"`
	Description        string          `json:"description"`
}

// ColumnMapping å­—æ®µæ˜ å°„
type ColumnMapping struct {
	OriginalName  string `json:"original_name"`
	OptimizedName string `json:"optimized_name"`
	Description   string `json:"description"`
}

// SemanticOptimizeDataSource å¯¹æ•°æ®æºè¿›è¡Œè¯­ä¹‰ä¼˜åŒ–
func (a *App) SemanticOptimizeDataSource(sourceID string) error {
	a.Log("Starting semantic optimization for data source: " + sourceID)

	// 1. è·å–åŸæ•°æ®æºä¿¡æ¯
	sources, err := a.dataSourceService.LoadDataSources()
	if err != nil {
		return fmt.Errorf("failed to load data sources: %w", err)
	}

	var originalSource *agent.DataSource
	for _, s := range sources {
		if s.ID == sourceID {
			originalSource = &s
			break
		}
	}

	if originalSource == nil {
		return fmt.Errorf("data source not found: %s", sourceID)
	}

	// å‘é€è¿›åº¦äº‹ä»?
	a.sendSemanticOptimizeProgress("æ­£åœ¨åˆ†æè¡¨ç»“æ?..")

	// 2. æ”¶é›†æ‰€æœ‰è¡¨çš„ç»“æ„å’Œæ ·æœ¬æ•°æ®
	schemas, err := a.collectTableSchemas(sourceID)
	if err != nil {
		return fmt.Errorf("failed to collect table schemas: %w", err)
	}

	if len(schemas) == 0 {
		return fmt.Errorf("no tables found in data source")
	}

	a.sendSemanticOptimizeProgress("æ­£åœ¨ç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ...")

	// 3. è°ƒç”¨ LLM ç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ
	cfg, err := a.GetConfig()
	if err != nil {
		return fmt.Errorf("failed to load config: %w", err)
	}

	optimization, err := a.generateSemanticOptimization(schemas, cfg.Language)
	if err != nil {
		return fmt.Errorf("failed to generate optimization: %w", err)
	}

	a.sendSemanticOptimizeProgress("æ­£åœ¨åˆ›å»ºæ–°æ•°æ®æº...")

	// 4. åˆ›å»ºæ–°æ•°æ®æºï¼ˆè¿”å›æ–°æ•°æ®æºå’Œå®Œæ•´æ•°æ®åº“è·¯å¾„ï¼‰
	// ä¼ é€?schemas ä»¥ä¾¿ä½¿ç”¨å·²æ”¶é›†çš„åˆ—ç±»å‹ä¿¡æ?
	newSource, newDBFullPath, err := a.createOptimizedDataSource(originalSource, optimization, schemas)
	if err != nil {
		return fmt.Errorf("failed to create optimized data source: %w", err)
	}

	a.sendSemanticOptimizeProgress("æ­£åœ¨è¿ç§»æ•°æ®...")

	// 5. è¿ç§»æ•°æ®ï¼ˆä½¿ç”¨å®Œæ•´è·¯å¾„ï¼‰
	err = a.migrateDataWithOptimization(originalSource, newDBFullPath, optimization)
	if err != nil {
		return fmt.Errorf("failed to migrate data: %w", err)
	}

	a.sendSemanticOptimizeProgress("å®Œæˆ")

	// 7. å‘é€å®Œæˆäº‹ä»?
	runtime.EventsEmit(a.ctx, "semantic-optimize-completed", map[string]interface{}{
		"original_id": sourceID,
		"new_id":      newSource.ID,
		"new_name":    newSource.Name,
	})

	a.Log("Semantic optimization completed successfully")
	return nil
}

// collectTableSchemas æ”¶é›†è¡¨ç»“æ„ä¿¡æ?
func (a *App) collectTableSchemas(sourceID string) ([]TableSchemaInfo, error) {
	// è·å–æ•°æ®æºä¿¡æ?
	sources, err := a.dataSourceService.LoadDataSources()
	if err != nil {
		a.Log(fmt.Sprintf("[SEMANTIC] Error loading data sources: %v", err))
		return nil, err
	}

	var ds *agent.DataSource
	for _, s := range sources {
		if s.ID == sourceID {
			ds = &s
			break
		}
	}

	if ds == nil {
		a.Log(fmt.Sprintf("[SEMANTIC] Data source not found: %s", sourceID))
		return nil, fmt.Errorf("data source not found: %s", sourceID)
	}

	a.Log(fmt.Sprintf("[SEMANTIC] Data source info - ID: %s, Name: %s, Type: %s, DBPath: %s",
		ds.ID, ds.Name, ds.Type, ds.Config.DBPath))

	// é¦–å…ˆå°è¯•ä»?analysis.schema è·å–è¡¨ä¿¡æ?
	if ds.Analysis != nil && len(ds.Analysis.Schema) > 0 {
		a.Log(fmt.Sprintf("[SEMANTIC] Using analysis.schema, found %d tables", len(ds.Analysis.Schema)))
		schemas := make([]TableSchemaInfo, 0, len(ds.Analysis.Schema))
		
		for _, tableSchema := range ds.Analysis.Schema {
			// å°è¯•è·å–çœŸå®çš„åˆ—ç±»å‹ä¿¡æ¯
			columnInfos := make([]ColumnInfo, 0, len(tableSchema.Columns))
			realColumns, err := a.dataSourceService.GetTableColumns(sourceID, tableSchema.TableName)
			if err == nil && len(realColumns) > 0 {
				// æˆåŠŸè·å–çœŸå®åˆ—ä¿¡æ?
				columnTypeMap := make(map[string]string)
				for _, col := range realColumns {
					columnTypeMap[col.Name] = col.Type
				}
				for _, colName := range tableSchema.Columns {
					colType := columnTypeMap[colName]
					if colType == "" {
						colType = "TEXT"
					}
					columnInfos = append(columnInfos, ColumnInfo{
						Name:     colName,
						Type:     colType,
						Nullable: true,
					})
				}
			} else {
				// æ— æ³•è·å–çœŸå®åˆ—ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹
				a.Log(fmt.Sprintf("[SEMANTIC] Could not get real column types for %s, using TEXT: %v", tableSchema.TableName, err))
				for _, colName := range tableSchema.Columns {
					columnInfos = append(columnInfos, ColumnInfo{
						Name:     colName,
						Type:     "TEXT",
						Nullable: true,
					})
				}
			}
			
			// å°è¯•è·å–æ ·æœ¬æ•°æ®
			sampleData, err := a.getSampleData(sourceID, tableSchema.TableName, 2)
			if err != nil {
				a.Log(fmt.Sprintf("Failed to get sample data for table %s: %v", tableSchema.TableName, err))
				sampleData = []map[string]any{}
			}
			
			schemas = append(schemas, TableSchemaInfo{
				TableName:  tableSchema.TableName,
				Columns:    columnInfos,
				SampleData: sampleData,
			})
		}
		
		return schemas, nil
	}

	// è·å–æ‰€æœ‰è¡¨å?
	a.Log(fmt.Sprintf("[SEMANTIC] Getting tables for source: %s", sourceID))
	tables, err := a.dataSourceService.GetTables(sourceID)
	if err != nil {
		a.Log(fmt.Sprintf("[SEMANTIC] Error getting tables: %v", err))
		return nil, err
	}
	a.Log(fmt.Sprintf("[SEMANTIC] Found %d tables: %v", len(tables), tables))

	// å¦‚æœ GetTables è¿”å›ç©ºï¼Œå°è¯•ä½¿ç”¨ GetDataSourceTables
	if len(tables) == 0 {
		a.Log("[SEMANTIC] GetTables returned empty, trying GetDataSourceTables...")
		tables, err = a.dataSourceService.GetDataSourceTables(sourceID)
		if err != nil {
			a.Log(fmt.Sprintf("[SEMANTIC] Error getting tables via GetDataSourceTables: %v", err))
			return nil, err
		}
		a.Log(fmt.Sprintf("[SEMANTIC] GetDataSourceTables found %d tables: %v", len(tables), tables))
	}

	schemas := make([]TableSchemaInfo, 0, len(tables))

	for _, tableName := range tables {
		// è·å–è¡¨ç»“æ?
		columns, err := a.dataSourceService.GetTableColumns(sourceID, tableName)
		if err != nil {
			a.Log(fmt.Sprintf("Failed to get columns for table %s: %v", tableName, err))
			continue
		}

		columnInfos := make([]ColumnInfo, 0, len(columns))
		for _, col := range columns {
			columnInfos = append(columnInfos, ColumnInfo{
				Name:     col.Name,
				Type:     col.Type,
				Nullable: col.Nullable,
			})
		}

		// è·å–å‰ä¸¤è¡Œæ ·æœ¬æ•°æ?
		sampleData, err := a.getSampleData(sourceID, tableName, 2)
		if err != nil {
			a.Log(fmt.Sprintf("Failed to get sample data for table %s: %v", tableName, err))
			sampleData = []map[string]any{} // ç»§ç»­å¤„ç†ï¼Œå³ä½¿æ²¡æœ‰æ ·æœ¬æ•°æ?
		}

		schemas = append(schemas, TableSchemaInfo{
			TableName:  tableName,
			Columns:    columnInfos,
			SampleData: sampleData,
		})
	}

	return schemas, nil
}

// getSampleData è·å–è¡¨çš„æ ·æœ¬æ•°æ®
func (a *App) getSampleData(sourceID, tableName string, limit int) ([]map[string]any, error) {
	query := fmt.Sprintf("SELECT * FROM %s LIMIT %d", tableName, limit)
	return a.dataSourceService.ExecuteQuery(sourceID, query)
}

// generateSemanticOptimization è°ƒç”¨ LLM ç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ
func (a *App) generateSemanticOptimization(schemas []TableSchemaInfo, language string) (*SemanticOptimizationResult, error) {
	// æ„å»º prompt
	prompt := a.buildSemanticOptimizationPrompt(schemas, language)

	// è·å–é…ç½®å¹¶åˆ›å»?LLM æœåŠ¡
	cfg, err := a.GetEffectiveConfig()
	if err != nil {
		return nil, fmt.Errorf("failed to get config: %w", err)
	}

	llm := agent.NewLLMService(cfg, a.Log)

	// è°ƒç”¨ LLM
	ctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)
	defer cancel()

	response, err := llm.Chat(ctx, prompt)
	if err != nil {
		return nil, fmt.Errorf("LLM call failed: %w", err)
	}

	// è§£æ JSON å“åº”
	var result SemanticOptimizationResult
	
	// å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å«åœ¨ markdown ä»£ç å—ä¸­ï¼?
	content := response
	
	// å…ˆå°è¯•æ‰¾ ```json ä»£ç å?
	if idx := strings.Index(content, "```json"); idx != -1 {
		// æ‰¾åˆ° ```json åçš„å†…å®¹
		start := idx + 7
		// è·³è¿‡å¯èƒ½çš„æ¢è¡Œç¬¦
		for start < len(content) && (content[start] == '\n' || content[start] == '\r') {
			start++
		}
		// æ‰¾åˆ°ç»“æŸçš?```
		remaining := content[start:]
		if endIdx := strings.Index(remaining, "```"); endIdx != -1 {
			content = remaining[:endIdx]
		} else {
			content = remaining
		}
	} else if idx := strings.Index(content, "```"); idx != -1 {
		// æ‰¾åˆ°æ™®é€?``` ä»£ç å?
		start := idx + 3
		// è·³è¿‡å¯èƒ½çš„æ¢è¡Œç¬¦
		for start < len(content) && (content[start] == '\n' || content[start] == '\r') {
			start++
		}
		remaining := content[start:]
		if endIdx := strings.Index(remaining, "```"); endIdx != -1 {
			content = remaining[:endIdx]
		} else {
			content = remaining
		}
	}

	content = strings.TrimSpace(content)
	
	// ç¡®ä¿å†…å®¹ä»?{ å¼€å¤´ï¼ˆæ‰¾åˆ°ç¬¬ä¸€ä¸?{ çš„ä½ç½®ï¼‰
	if idx := strings.Index(content, "{"); idx > 0 {
		content = content[idx:]
	}
	
	// ç¡®ä¿å†…å®¹ä»?} ç»“å°¾ï¼ˆæ‰¾åˆ°æœ€åä¸€ä¸?} çš„ä½ç½®ï¼‰
	if idx := strings.LastIndex(content, "}"); idx != -1 && idx < len(content)-1 {
		content = content[:idx+1]
	}

	err = json.Unmarshal([]byte(content), &result)
	if err != nil {
		return nil, fmt.Errorf("failed to parse LLM response: %w\nResponse: %s", err, content)
	}

	return &result, nil
}

// buildSemanticOptimizationPrompt æ„å»ºè¯­ä¹‰ä¼˜åŒ– prompt
func (a *App) buildSemanticOptimizationPrompt(schemas []TableSchemaInfo, language string) string {
	// åºåˆ—åŒ–è¡¨ç»“æ„ä¿¡æ¯
	schemasJSON, _ := json.MarshalIndent(schemas, "", "  ")

	langInstruction := "Use English for field names"
	if language == "ç®€ä½“ä¸­æ–? {
		langInstruction = "ä½¿ç”¨è‹±æ–‡å‘½åå­—æ®µï¼Œä½†è¦è€ƒè™‘ä¸­æ–‡ä¸šåŠ¡è¯­ä¹‰"
	}

	prompt := fmt.Sprintf(`# æ•°æ®æºè¯­ä¹‰ä¼˜åŒ–ä»»åŠ?

ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“è®¾è®¡ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ•°æ®è¡¨çš„ç»“æ„å’Œæ ·æœ¬æ•°æ®ï¼Œä¸ºæ¯ä¸ªå­—æ®µç”Ÿæˆæ›´æœ‰æ„ä¹‰ã€æ›´æ˜“ç†è§£çš„å­—æ®µåã€?

## è¦æ±‚

1. **å­—æ®µåè§„èŒ?*ï¼?
   - ä½¿ç”¨å°å†™å­—æ¯å’Œä¸‹åˆ’çº¿ï¼ˆsnake_caseï¼?
   - åç§°è¦ç®€æ´ä½†æœ‰æ„ä¹?
   - é¿å…ç¼©å†™ï¼Œé™¤éæ˜¯é€šç”¨ç¼©å†™ï¼ˆå¦‚ id, url, qtyï¼?
   - %s

2. **ä¿æŒæ•°æ®ç±»å‹**ï¼?
   - ä¸æ”¹å˜å­—æ®µçš„æ•°æ®ç±»å‹
   - ä¿æŒå­—æ®µçš„å¯ç©ºæ€?

3. **è€ƒè™‘ä¸šåŠ¡è¯­ä¹‰**ï¼?
   - æ ¹æ®æ ·æœ¬æ•°æ®æ¨æ–­å­—æ®µçš„ä¸šåŠ¡å«ä¹?
   - ä½¿ç”¨ä¸šåŠ¡æœ¯è¯­è€ŒéæŠ€æœ¯æœ¯è¯?
   - è€ƒè™‘å­—æ®µä¹‹é—´çš„å…³è”å…³ç³?

4. **è¡¨åä¼˜åŒ–**ï¼?
   - å¦‚æœè¡¨åä¸å¤Ÿæ¸…æ™°ï¼Œä¹Ÿå¯ä»¥ä¼˜åŒ–è¡¨å
   - è¡¨ååº”è¯¥åæ˜ è¡¨çš„ä¸šåŠ¡å«ä¹‰
   - ä½¿ç”¨ snake_case å‘½å

## è¾“å…¥æ•°æ®

%s

## è¾“å‡ºæ ¼å¼

è¿”å› JSON æ ¼å¼ï¼Œç»“æ„å¦‚ä¸‹ï¼š

{
  "tables": [
    {
      "original_table_name": "åŸè¡¨å?,
      "optimized_table_name": "ä¼˜åŒ–åçš„è¡¨å",
      "description": "è¡¨çš„ä¸šåŠ¡æè¿°",
      "column_mappings": [
        {
          "original_name": "åŸå­—æ®µå",
          "optimized_name": "ä¼˜åŒ–åçš„å­—æ®µå?,
          "description": "å­—æ®µçš„ä¸šåŠ¡å«ä¹?
        }
      ]
    }
  ]
}

## é‡è¦æç¤º

- åªè¿”å›?JSONï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜æ–‡å­?
- ç¡®ä¿æ‰€æœ‰åŸå§‹å­—æ®µéƒ½æœ‰å¯¹åº”çš„æ˜ å°„
- å¦‚æœå­—æ®µåå·²ç»å¾ˆå¥½ï¼Œå¯ä»¥ä¿æŒä¸å˜
- ä¼˜åŒ–åçš„å­—æ®µåå¿…é¡»æ˜¯æœ‰æ•ˆçš?SQL æ ‡è¯†ç¬?

è¯·å¼€å§‹ä¼˜åŒ–ï¼š`, langInstruction, string(schemasJSON))

	return prompt
}

// createOptimizedDataSource åˆ›å»ºä¼˜åŒ–åçš„æ•°æ®æº?
func (a *App) createOptimizedDataSource(originalSource *agent.DataSource, optimization *SemanticOptimizationResult, schemas []TableSchemaInfo) (*agent.DataSource, string, error) {
	// åˆ›å»ºæ–°çš„æ•°æ®æºåç§?
	newName := originalSource.Name + "_è¯­ä¹‰ä¼˜åŒ–"

	// åˆ›å»ºæ–°çš„ DuckDB æ•°æ®åº“æ–‡ä»¶ï¼ˆè¿”å›å®Œæ•´è·¯å¾„ï¼?
	newDBFullPath, err := a.dataSourceService.CreateOptimizedDatabase(originalSource, newName)
	if err != nil {
		return nil, "", err
	}

	// æ‰“å¼€æ–°æ•°æ®åº“
	newDB, err := a.dataSourceService.DB.OpenNew(newDBFullPath)
	if err != nil {
		return nil, "", fmt.Errorf("failed to open new database: %w", err)
	}
	defer newDB.Close()

	// æ„å»ºè¡¨ååˆ?schema çš„æ˜ å°„ï¼Œç”¨äºè·å–åˆ—ç±»å?
	schemaMap := make(map[string]TableSchemaInfo)
	for _, schema := range schemas {
		schemaMap[schema.TableName] = schema
	}

	// åˆ›å»ºä¼˜åŒ–åçš„è¡¨ç»“æ?
	for _, tableOpt := range optimization.Tables {
		// ä»?schemas ä¸­è·å–åŸè¡¨çš„åˆ—ç±»å‹ä¿¡æ?
		originalSchema, hasSchema := schemaMap[tableOpt.OriginalTableName]
		err = a.createOptimizedTable(newDB, tableOpt, originalSchema, hasSchema)
		if err != nil {
			return nil, "", fmt.Errorf("failed to create table %s: %w", tableOpt.OptimizedTableName, err)
		}
	}

	// ä»å®Œæ•´è·¯å¾„ä¸­æå–ç›¸å¯¹è·¯å¾„ï¼ˆsources/{id}/data.dbï¼?
	// å®Œæ•´è·¯å¾„æ ¼å¼ï¼š{dataCacheDir}/sources/{id}/data.db
	cfg, err := a.GetConfig()
	if err != nil {
		return nil, "", fmt.Errorf("failed to get config: %w", err)
	}
	
	// è®¡ç®—ç›¸å¯¹è·¯å¾„
	relDBPath, err := filepath.Rel(cfg.DataCacheDir, newDBFullPath)
	if err != nil {
		// å¦‚æœæ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•ä»è·¯å¾„ä¸­æå?sources/{id}/data.db éƒ¨åˆ†
		parts := strings.Split(filepath.ToSlash(newDBFullPath), "/")
		for i, part := range parts {
			if part == "sources" && i+2 < len(parts) {
				relDBPath = filepath.Join("sources", parts[i+1], parts[i+2])
				break
			}
		}
		if relDBPath == "" {
			relDBPath = filepath.Base(newDBFullPath)
		}
	}

	a.Log(fmt.Sprintf("[SEMANTIC] New database relative path: %s", relDBPath))

	// æ„å»ºæ–°æ•°æ®æºçš?schema ä¿¡æ¯
	newSchema := make([]agent.TableSchema, 0, len(optimization.Tables))
	for _, tableOpt := range optimization.Tables {
		columns := make([]string, 0, len(tableOpt.ColumnMappings))
		for _, mapping := range tableOpt.ColumnMappings {
			columns = append(columns, mapping.OptimizedName)
		}
		newSchema = append(newSchema, agent.TableSchema{
			TableName: tableOpt.OptimizedTableName,
			Columns:   columns,
		})
	}

	// æ„å»ºæ•°æ®æ‘˜è¦æè¿°
	var summaryParts []string
	for _, tableOpt := range optimization.Tables {
		if tableOpt.Description != "" {
			summaryParts = append(summaryParts, fmt.Sprintf("%s: %s", tableOpt.OptimizedTableName, tableOpt.Description))
		}
	}
	summary := strings.Join(summaryParts, "\n")
	if summary == "" {
		summary = i18n.T("datasource.semantic_opt_summary", len(optimization.Tables))
	}

	// æ³¨å†Œæ–°æ•°æ®æºï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ŒåŒ…å« schema ä¿¡æ¯ï¼?
	newSource := &agent.DataSource{
		ID:   generateID(),
		Name: newName,
		Type: "sqlite",
		Config: agent.DataSourceConfig{
			DBPath:    relDBPath,
			Optimized: true,
		},
		Analysis: &agent.DataSourceAnalysis{
			Summary: summary,
			Schema:  newSchema,
		},
	}

	err = a.dataSourceService.SaveDataSource(newSource)
	if err != nil {
		return nil, "", fmt.Errorf("failed to save new data source: %w", err)
	}

	// è¿”å›æ–°æ•°æ®æºå’Œå®Œæ•´è·¯å¾„ï¼ˆç”¨äºæ•°æ®è¿ç§»ï¼?
	return newSource, newDBFullPath, nil
}

// validateAndFixSQL ä½¿ç”¨ LLM éªŒè¯å’Œä¿®å¤?SQL è¯­å¥
func (a *App) validateAndFixSQL(sqlStmt string, tableName string) (string, error) {
	// ç”±äºæˆ‘ä»¬å·²ç»ä½¿ç”¨åŒå¼•å·åŒ…è£¹äº†æ‰€æœ‰æ ‡è¯†ç¬¦ï¼ŒSQL åº”è¯¥æ˜¯æ­£ç¡®çš„
	// ç›´æ¥è¿”å›åŸå§‹ SQLï¼Œé¿å…?LLM æ·»åŠ é¢å¤–å†…å®¹å¯¼è‡´è¯­æ³•é”™è¯¯
	a.Log(fmt.Sprintf("SQL for table %s: %s", tableName, sqlStmt))
	return sqlStmt, nil
}

// createOptimizedTable åˆ›å»ºä¼˜åŒ–åçš„è¡?
// ä½¿ç”¨å·²æ”¶é›†çš„ schema ä¿¡æ¯ï¼Œé¿å…å†æ¬¡æŸ¥è¯¢åŸæ•°æ®åº?
func (a *App) createOptimizedTable(db *sql.DB, tableOpt TableOptimization, originalSchema TableSchemaInfo, hasSchema bool) error {
	// æ„å»ºåˆ—å®šä¹‰æ˜ å°„ï¼ˆä»å·²æ”¶é›†çš?schema ä¸­è·å–ç±»å‹ï¼‰
	columnTypeMap := make(map[string]string)
	if hasSchema {
		for _, col := range originalSchema.Columns {
			columnTypeMap[col.Name] = col.Type
		}
	}

	// æ„å»º CREATE TABLE è¯­å¥
	var columnDefs []string
	for _, mapping := range tableOpt.ColumnMappings {
		colType := columnTypeMap[mapping.OriginalName]
		if colType == "" {
			colType = "TEXT" // é»˜è®¤ç±»å‹
		}
		// ä½¿ç”¨åŒå¼•å·åŒ…è£¹åˆ—åï¼Œé¿å…ä¿ç•™å­—é—®é¢?
		columnDefs = append(columnDefs, fmt.Sprintf(`"%s" %s`, mapping.OptimizedName, colType))
	}

	// ä½¿ç”¨åŒå¼•å·åŒ…è£¹è¡¨å?
	createSQL := fmt.Sprintf(`CREATE TABLE IF NOT EXISTS "%s" (%s)`,
		tableOpt.OptimizedTableName,
		strings.Join(columnDefs, ", "))

	// ä½¿ç”¨ LLM éªŒè¯å’Œä¿®å¤?SQL
	fixedSQL, err := a.validateAndFixSQL(createSQL, tableOpt.OptimizedTableName)
	if err != nil {
		a.Log("SQL validation error: " + err.Error())
		// ç»§ç»­ä½¿ç”¨åŸå§‹ SQL
		fixedSQL = createSQL
	}

	a.Log(fmt.Sprintf("Creating table with SQL: %s", fixedSQL))

	_, err = db.Exec(fixedSQL)
	if err != nil {
		// å¦‚æœæ‰§è¡Œå¤±è´¥ï¼Œè®°å½•è¯¦ç»†é”™è¯?
		return fmt.Errorf("failed to create table %s: %w\nSQL: %s", tableOpt.OptimizedTableName, err, fixedSQL)
	}
	return nil
}

// migrateDataWithOptimization ä½¿ç”¨ä¼˜åŒ–æ–¹æ¡ˆè¿ç§»æ•°æ®
func (a *App) migrateDataWithOptimization(originalSource *agent.DataSource, targetDBPath string, optimization *SemanticOptimizationResult) error {
	// æ‰“å¼€åŸæ•°æ®åº“
	sourceDB, err := a.dataSourceService.GetConnection(originalSource.ID)
	if err != nil {
		return fmt.Errorf("failed to connect to source database: %w", err)
	}
	defer sourceDB.Close()

	// æ‰“å¼€æ–°æ•°æ®åº“ï¼ˆä½¿ç”¨å®Œæ•´è·¯å¾„ï¼‰
	a.Log(fmt.Sprintf("Opening target database: %s", targetDBPath))
	targetDB, err := a.dataSourceService.DB.OpenWritable(targetDBPath)
	if err != nil {
		return fmt.Errorf("failed to open target database: %w", err)
	}
	defer targetDB.Close()

	// ç¡®å®šæºæ•°æ®åº“ç±»å‹ï¼Œç”¨äºæ­£ç¡®çš„æ ‡è¯†ç¬¦å¼•ç”?
	sourceDBType := originalSource.Type // "mysql", "sqlite", "excel", "csv", etc.
	
	// è¿ç§»æ¯ä¸ªè¡¨çš„æ•°æ®
	for _, tableOpt := range optimization.Tables {
		err = a.migrateTableData(sourceDB, targetDB, tableOpt, sourceDBType)
		if err != nil {
			return fmt.Errorf("failed to migrate table %s: %w", tableOpt.OriginalTableName, err)
		}
	}

	return nil
}

// quoteIdentifier æ ¹æ®æ•°æ®åº“ç±»å‹è¿”å›æ­£ç¡®çš„æ ‡è¯†ç¬¦å¼•ç”?
func quoteIdentifier(name string, dbType string) string {
	switch dbType {
	case "mysql", "doris":
		return fmt.Sprintf("`%s`", name)
	default:
		// DuckDB, PostgreSQL ä½¿ç”¨åŒå¼•å?
		return fmt.Sprintf(`"%s"`, name)
	}
}

// migrateTableData è¿ç§»å•ä¸ªè¡¨çš„æ•°æ®
func (a *App) migrateTableData(sourceDB, targetDB *sql.DB, tableOpt TableOptimization, sourceDBType string) error {
	// æ„å»ºå­—æ®µåˆ—è¡¨
	originalCols := make([]string, 0, len(tableOpt.ColumnMappings))
	optimizedCols := make([]string, 0, len(tableOpt.ColumnMappings))

	for _, mapping := range tableOpt.ColumnMappings {
		// åŸè¡¨åˆ—åä½¿ç”¨æºæ•°æ®åº“çš„å¼•ç”¨æ–¹å¼?
		originalCols = append(originalCols, quoteIdentifier(mapping.OriginalName, sourceDBType))
		// æ–°è¡¨åˆ—åä½¿ç”¨ DuckDB çš„å¼•ç”¨æ–¹å¼ï¼ˆåŒå¼•å·ï¼‰
		optimizedCols = append(optimizedCols, quoteIdentifier(mapping.OptimizedName, "duckdb"))
	}

	// æŸ¥è¯¢åŸè¡¨æ•°æ®ï¼ˆä½¿ç”¨æºæ•°æ®åº“çš„å¼•ç”¨æ–¹å¼ï¼?
	query := fmt.Sprintf(`SELECT %s FROM %s`,
		strings.Join(originalCols, ", "),
		quoteIdentifier(tableOpt.OriginalTableName, sourceDBType))

	a.Log(fmt.Sprintf("Querying source table: %s", query))

	rows, err := sourceDB.Query(query)
	if err != nil {
		return fmt.Errorf("failed to query source table: %w", err)
	}
	defer rows.Close()

	// å‡†å¤‡æ’å…¥è¯­å¥
	placeholders := make([]string, len(optimizedCols))
	for i := range placeholders {
		placeholders[i] = "?"
	}

	insertQuery := fmt.Sprintf(`INSERT INTO "%s" (%s) VALUES (%s)`,
		tableOpt.OptimizedTableName,
		strings.Join(optimizedCols, ", "),
		strings.Join(placeholders, ", "))

	a.Log(fmt.Sprintf("Insert query: %s", insertQuery))

	// å¼€å§‹äº‹åŠ?
	tx, err := targetDB.Begin()
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}

	// åœ¨äº‹åŠ¡ä¸­å‡†å¤‡è¯­å¥
	stmt, err := tx.Prepare(insertQuery)
	if err != nil {
		tx.Rollback()
		return fmt.Errorf("failed to prepare insert statement: %w", err)
	}
	defer stmt.Close()

	// é€è¡Œè¿ç§»
	rowCount := 0
	for rows.Next() {
		values := make([]interface{}, len(originalCols))
		valuePtrs := make([]interface{}, len(originalCols))
		for i := range values {
			valuePtrs[i] = &values[i]
		}

		if err := rows.Scan(valuePtrs...); err != nil {
			tx.Rollback()
			return fmt.Errorf("failed to scan row: %w", err)
		}

		if _, err := stmt.Exec(values...); err != nil {
			tx.Rollback()
			return fmt.Errorf("failed to insert row: %w", err)
		}

		rowCount++
	}

	if err := rows.Err(); err != nil {
		tx.Rollback()
		return fmt.Errorf("error iterating rows: %w", err)
	}

	// æäº¤äº‹åŠ¡
	if err := tx.Commit(); err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}

	a.Log(fmt.Sprintf("Migrated %d rows from %s to %s", rowCount, tableOpt.OriginalTableName, tableOpt.OptimizedTableName))
	return nil
}

// sendSemanticOptimizeProgress å‘é€è¿›åº¦äº‹ä»?
func (a *App) sendSemanticOptimizeProgress(message string) {
	runtime.EventsEmit(a.ctx, "semantic-optimize-progress", map[string]interface{}{
		"message": message,
	})
}

// generateID ç”Ÿæˆå”¯ä¸€ ID
func generateID() string {
	return fmt.Sprintf("ds_%d", time.Now().UnixNano())
}
