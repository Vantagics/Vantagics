package agent

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"time"

	"github.com/cloudwego/eino-ext/components/model/openai"
	"github.com/cloudwego/eino/components/model"
	"github.com/cloudwego/eino/components/tool"
	"github.com/cloudwego/eino/compose"
	"github.com/cloudwego/eino/schema"

	"rapidbi/agent/templates"
	"rapidbi/config"
)

// getProviderMaxTokens returns the maximum OUTPUT tokens for different providers
// This controls how long the LLM's response can be, NOT the total context window
func getProviderMaxTokens(modelName string, configuredMax int) int {
	// Provider-specific OUTPUT limits based on model names
	// These are conservative limits to ensure complete responses
	providerLimits := map[string]int{
		// OpenAI models - output limits
		"gpt-4":           8192,
		"gpt-4-turbo":     16384,  // Increased for longer outputs
		"gpt-4o":          16384,  // Increased for longer outputs
		"gpt-3.5-turbo":   4096,
		
		// Anthropic models - output limits
		"claude-3":        8192,
		"claude-3-sonnet": 8192,
		"claude-3-opus":   8192,
		"claude-3-haiku":  8192,
		
		// Google Gemini models - output limits
		"gemini-3-pro":         16384,
		"gemini-3-flash":       16384,
		"gemini-2.5-pro":       16384,
		"gemini-2.5-flash":     16384,
		
		// Default fallback
		"default":         8192,
	}
	
	// Find the limit for this model
	limit := providerLimits["default"]
	for model, maxTokens := range providerLimits {
		if strings.Contains(strings.ToLower(modelName), strings.ToLower(model)) {
			limit = maxTokens
			break
		}
	}
	
	// If configured max is set and reasonable, use it
	if configuredMax > 0 && configuredMax <= limit {
		return configuredMax
	}
	
	// Otherwise use the provider's limit
	return limit
}

// normalizeOpenAIBaseURL normalizes the base URL for OpenAI-compatible APIs
// The OpenAI SDK automatically appends /chat/completions, so we need to strip it if present
// This allows users to enter either:
//   - https://api.example.com/v1 (correct)
//   - https://api.example.com/v1/chat/completions (also works after normalization)
func normalizeOpenAIBaseURL(baseURL string) string {
	if baseURL == "" {
		return baseURL
	}
	
	// Remove trailing slash first
	baseURL = strings.TrimSuffix(baseURL, "/")
	
	// Remove /chat/completions suffix if present (SDK will add it back)
	if strings.HasSuffix(baseURL, "/chat/completions") {
		baseURL = strings.TrimSuffix(baseURL, "/chat/completions")
	}
	
	// Also handle case where user might have added just /completions
	if strings.HasSuffix(baseURL, "/completions") {
		baseURL = strings.TrimSuffix(baseURL, "/completions")
	}
	
	return baseURL
}

// EinoService manages Eino-based agents
type EinoService struct {
	ChatModel                  model.ChatModel
	dsService                  *DataSourceService
	cfg                        config.Config
	Logger                     func(string)
	memoryManager              *MemoryManager
	workingContextManager      *WorkingContextManager
	conversationContextManager *ConversationContextManager // For tracking conversation context
	pythonPool                 *PythonPool
	errorKnowledge             *ErrorKnowledge
	skillManager               *templates.SkillManager
	memoryService              *MemoryService // For persistent memory storage
	executionValidator         *ExecutionValidator // For execution plan validation
}

// TrajectoryStep represents a single step in agent execution
type TrajectoryStep struct {
	StepNumber  int                      `json:"step_number"`
	Timestamp   int64                    `json:"timestamp"`
	Type        string                   `json:"type"` // "model_call" | "tool_call"
	ModelInput  []map[string]interface{} `json:"model_input,omitempty"`
	ModelOutput map[string]interface{}   `json:"model_output,omitempty"`
	ToolName    string                   `json:"tool_name,omitempty"`
	ToolInput   string                   `json:"tool_input,omitempty"`
	ToolOutput  string                   `json:"tool_output,omitempty"`
	ToolCallID  string                   `json:"tool_call_id,omitempty"`
	Error       string                   `json:"error,omitempty"`
}

// AgentTrajectory represents complete execution path for training
type AgentTrajectory struct {
	ThreadID       string           `json:"thread_id"`
	UserRequest    string           `json:"user_request"`
	DataSourceID   string           `json:"data_source_id,omitempty"`
	StartTime      int64            `json:"start_time"`
	EndTime        int64            `json:"end_time"`
	TotalDuration  int64            `json:"total_duration_ms"`
	Steps          []TrajectoryStep `json:"steps"`
	FinalResponse  string           `json:"final_response"`
	Success        bool             `json:"success"`
	ErrorMessage   string           `json:"error_message,omitempty"`
	IterationCount int              `json:"iteration_count"`
	ToolCallCount  int              `json:"tool_call_count"`
}

// NewEinoService creates a new EinoService
func NewEinoService(cfg config.Config, dsService *DataSourceService, memoryService *MemoryService, workingContextManager *WorkingContextManager, logger func(string)) (*EinoService, error) {
	// Validate required configuration
	if cfg.ModelName == "" {
		return nil, fmt.Errorf("model name is required but not configured")
	}
	
	if logger != nil {
		logger(fmt.Sprintf("[EINO-INIT] Creating EinoService with provider: %s, model: %s", cfg.LLMProvider, cfg.ModelName))
	}
	
	var chatModel model.ChatModel
	var err error

	switch cfg.LLMProvider {
	case "Anthropic":
		if logger != nil {
			logger(fmt.Sprintf("[EINO-INIT] Initializing Anthropic model: %s", cfg.ModelName))
		}
		chatModel, err = NewAnthropicChatModel(context.Background(), &AnthropicConfig{
			APIKey:    cfg.APIKey,
			BaseURL:   cfg.BaseURL,
			Model:     cfg.ModelName,
			MaxTokens: cfg.MaxTokens,
		})
	case "Gemini":
		if logger != nil {
			logger(fmt.Sprintf("[EINO-INIT] Initializing Gemini model: %s", cfg.ModelName))
		}
		chatModel, err = NewGeminiChatModel(context.Background(), &GeminiConfig{
			APIKey:    cfg.APIKey,
			BaseURL:   cfg.BaseURL,
			Model:     cfg.ModelName,
			MaxTokens: cfg.MaxTokens,
		})
	default:
		// Default to OpenAI (includes "OpenAI", "OpenAI-Compatible", "Claude-Compatible" if using OAI format)
		// Note: "Claude-Compatible" in this project usually means "Use OpenAI client but point to Claude proxy"
		// or "Use Anthropic client". 
		// If LLMService treats Claude-Compatible as Anthropic-format, we should use AnthropicChatModel.
		// Checking llm_service.go: Claude-Compatible uses /v1/messages. So it is Anthropic format.
		if cfg.LLMProvider == "Claude-Compatible" {
			if logger != nil {
				logger(fmt.Sprintf("[EINO-INIT] Initializing Claude-Compatible model: %s", cfg.ModelName))
			}
			chatModel, err = NewAnthropicChatModel(context.Background(), &AnthropicConfig{
				APIKey:    cfg.APIKey,
				BaseURL:   cfg.BaseURL,
				Model:     cfg.ModelName,
				MaxTokens: cfg.MaxTokens,
			})
		} else {
			if logger != nil {
				logger(fmt.Sprintf("[EINO-INIT] Initializing OpenAI-Compatible model: %s", cfg.ModelName))
			}
			
			// Validate OpenAI configuration
			if cfg.APIKey == "" {
				return nil, fmt.Errorf("OpenAI API key is empty - please configure your API key")
			}
			
			// Set max tokens for OpenAI with intelligent provider limits
			maxTokens := getProviderMaxTokens(cfg.ModelName, cfg.MaxTokens)
			
			// Normalize BaseURL - OpenAI SDK automatically appends /chat/completions
			// so we need to strip it if user included it in the URL
			normalizedBaseURL := normalizeOpenAIBaseURL(cfg.BaseURL)
			if logger != nil && normalizedBaseURL != cfg.BaseURL {
				logger(fmt.Sprintf("[EINO-INIT] Normalized BaseURL: %s -> %s", cfg.BaseURL, normalizedBaseURL))
			}
			
			chatModel, err = openai.NewChatModel(context.Background(), &openai.ChatModelConfig{
				APIKey:    cfg.APIKey,
				BaseURL:   normalizedBaseURL,
				Model:     cfg.ModelName,
				MaxTokens: &maxTokens, // Use pointer to int
				Timeout:   0, // Default
			})
		}
	}

	if err != nil {
		if logger != nil {
			logger(fmt.Sprintf("[EINO-INIT] Failed to create chat model: %v", err))
		}
		return nil, fmt.Errorf("failed to create eino chat model: %v", err)
	}

	if logger != nil {
		logger(fmt.Sprintf("[EINO-INIT] Chat model created successfully"))
	}

	// Initialize memory manager with config's MaxTokens
	memManager := NewMemoryManager(cfg.MaxTokens, chatModel)

	// Initialize Python pool if Python path is configured
	var pyPool *PythonPool
	if cfg.PythonPath != "" {
		pool, err := NewPythonPool(cfg.PythonPath, 2)
		if err != nil {
			if logger != nil {
				logger(fmt.Sprintf("[WARNING] Failed to create Python pool: %v. Will use fallback execution.", err))
			}
		} else {
			pyPool = pool
			if logger != nil {
				logger("[INFO] Python process pool initialized")
			}
		}
	}

	// Initialize error knowledge system
	errorKnowledge := NewErrorKnowledge(dsService.dataCacheDir, logger)
	if logger != nil {
		logger("[INFO] Error knowledge system initialized")
	}

	// Initialize Skills Manager
	skillsDir := filepath.Join(dsService.dataCacheDir, "..", "skills") // Skills in RapidBI/skills
	skillManager := templates.NewSkillManager(skillsDir, logger)
	if err := skillManager.LoadSkills(); err != nil {
		if logger != nil {
			logger(fmt.Sprintf("[WARNING] Failed to load skills: %v", err))
		}
	}

	// Initialize Execution Validator
	executionValidator := NewExecutionValidator(logger)
	if logger != nil {
		logger("[INFO] Execution Validator initialized")
	}

	// Initialize Conversation Context Manager
	conversationContextManager := NewConversationContextManager()
	if logger != nil {
		logger("[INFO] Conversation Context Manager initialized")
	}

	return &EinoService{
		ChatModel:                  chatModel,
		dsService:                  dsService,
		cfg:                        cfg,
		Logger:                     logger,
		memoryManager:              memManager,
		workingContextManager:      workingContextManager,
		conversationContextManager: conversationContextManager,
		pythonPool:                 pyPool,
		errorKnowledge:             errorKnowledge,
		skillManager:               skillManager,
		memoryService:              memoryService,
		executionValidator:         executionValidator,
	}, nil
}

// RunAgent is a placeholder for running an Eino graph/chain
func (s *EinoService) RunAgent(ctx context.Context, input string) (string, error) {
	// Example: Simple chain
	// In a real scenario, we would build a graph with tools, memory, etc.
	
	chain := compose.NewChain[*schema.Message, *schema.Message]()
	chain.AppendChatModel(s.ChatModel)
	
	runnable, err := chain.Compile(ctx)
	if err != nil {
		return "", err
	}

	msg := &schema.Message{
		Role:    schema.User,
		Content: input,
	}

	resp, err := runnable.Invoke(ctx, msg)
	if err != nil {
		return "", err
	}

	return resp.Content, nil
}

// Close cleans up resources (Python pool, etc.)
func (s *EinoService) Close() {
	if s.pythonPool != nil {
		s.pythonPool.Close()
		s.pythonPool = nil
		if s.Logger != nil {
			s.Logger("[INFO] Python pool closed")
		}
	}
}

// GetErrorKnowledge returns the error knowledge instance
func (s *EinoService) GetErrorKnowledge() *ErrorKnowledge {
	return s.errorKnowledge
}

// GetSkillManager returns the skill manager instance
func (s *EinoService) GetSkillManager() *templates.SkillManager {
	return s.skillManager
}

// GetConfig returns the configuration
func (s *EinoService) GetConfig() config.Config {
	return s.cfg
}

// GetExecutionValidator returns the execution validator instance
func (s *EinoService) GetExecutionValidator() *ExecutionValidator {
	return s.executionValidator
}

// RunAnalysis executes the agent with full history and tool support
func (s *EinoService) RunAnalysis(ctx context.Context, history []*schema.Message, dataSourceID, threadID string) (*schema.Message, error) {
	return s.RunAnalysisWithProgress(ctx, history, dataSourceID, threadID, "", "", nil, nil, nil)
}

// RunAnalysisWithProgress executes the agent with progress callbacks
func (s *EinoService) RunAnalysisWithProgress(ctx context.Context, history []*schema.Message, dataSourceID, threadID, sessionDir, userMessageID string, onProgress ProgressCallback, onFileSaved func(fileName, fileType string, fileSize int64), cancelCheck func() bool) (*schema.Message, error) {
	startTotal := time.Now()
	if s.Logger != nil {
		s.Logger(fmt.Sprintf("[TIMING] Start RunAnalysis for thread: %s", threadID))
	}

	// Configure memory manager with memory service for this thread (only if memory is enabled)
	if s.cfg.EnableMemory && s.memoryManager != nil && s.memoryService != nil && threadID != "" {
		s.memoryManager.SetMemoryService(s.memoryService, threadID)
		if s.Logger != nil {
			s.Logger("[MEMORY] Memory service configured for thread")
		}
	} else if s.Logger != nil && !s.cfg.EnableMemory {
		s.Logger("[MEMORY] Memory feature disabled in config")
	}

	// Initialize trajectory tracking for training
	trajectory := &AgentTrajectory{
		ThreadID:     threadID,
		DataSourceID: dataSourceID,
		StartTime:    time.Now().UnixMilli(),
		Steps:        []TrajectoryStep{},
		Success:      false,
	}

	// Extract user request from last message with escaping for training visibility
	var lastUserMessage string
	if len(history) > 0 {
		for i := len(history) - 1; i >= 0; i-- {
			if history[i].Role == schema.User {
				trajectory.UserRequest = escapeForTraining(history[i].Content)
				lastUserMessage = history[i].Content
				break
			}
		}
	}

	// Update conversation context with user message
	if s.conversationContextManager != nil && threadID != "" && lastUserMessage != "" {
		s.conversationContextManager.UpdateFromUserMessage(threadID, lastUserMessage)
		
		// Resolve references in user message (e.g., "å¤©æ°”" -> "åŒ—äº¬çš„å¤©æ°”")
		resolvedMessage := s.conversationContextManager.ResolveReferences(threadID, lastUserMessage)
		if resolvedMessage != lastUserMessage {
			if s.Logger != nil {
				s.Logger(fmt.Sprintf("[CONTEXT] Resolved message: %s -> %s", lastUserMessage, resolvedMessage))
			}
			// Update the last user message in history with resolved version
			for i := len(history) - 1; i >= 0; i-- {
				if history[i].Role == schema.User {
					history[i].Content = resolvedMessage
					break
				}
			}
		}
	}

	// Initialize SQL collector for this session
	var sqlCollector *SQLCollector
	if sessionDir != "" && dataSourceID != "" {
		// Get data source name
		var dataSourceName string
		if sources, err := s.dsService.LoadDataSources(); err == nil {
			for _, ds := range sources {
				if ds.ID == dataSourceID {
					dataSourceName = ds.Name
					break
				}
			}
		}
		sqlCollector = NewSQLCollector(threadID, dataSourceID, dataSourceName)
		if s.Logger != nil {
			s.Logger("[SQL-COLLECTOR] Initialized for session")
		}
	}
	
	// Initialize execution recorder for this session
	var executionRecorder *ExecutionRecorder
	if sessionDir != "" && dataSourceID != "" {
		// Get data source name
		var dataSourceName string
		if sources, err := s.dsService.LoadDataSources(); err == nil {
			for _, ds := range sources {
				if ds.ID == dataSourceID {
					dataSourceName = ds.Name
					break
				}
			}
		}
		
		// Extract user request from history
		var userRequest string
		if len(history) > 0 {
			for i := len(history) - 1; i >= 0; i-- {
				if history[i].Role == schema.User {
					userRequest = history[i].Content
					break
				}
			}
		}
		
		executionRecorder = NewExecutionRecorder(sessionDir, dataSourceID, dataSourceName, userRequest, userMessageID, s.Logger)
		if s.Logger != nil {
			s.Logger("[EXECUTION-RECORDER] Initialized for session")
		}
	}

	// Save trajectory and SQL collection data on completion (success or error)
	defer func() {
		// Recover from any panic and record it
		if r := recover(); r != nil {
			if s.Logger != nil {
				s.Logger(fmt.Sprintf("[PANIC] Recovered from panic in RunAnalysisWithProgress: %v", r))
			}
			trajectory.Success = false
			trajectory.ErrorMessage = fmt.Sprintf("panic: %v", r)
		}
		
		// Record end time and duration
		trajectory.EndTime = time.Now().UnixMilli()
		trajectory.TotalDuration = trajectory.EndTime - trajectory.StartTime
		// Note: iterationCount is updated in trajectory.IterationCount during execution
		
		if sessionDir != "" {
			s.saveTrajectory(sessionDir, trajectory)
			
			// Save SQL collection data
			if sqlCollector != nil {
				if err := sqlCollector.SaveToFile(sessionDir); err != nil {
					if s.Logger != nil {
						s.Logger(fmt.Sprintf("[SQL-COLLECTOR] Failed to save: %v", err))
					}
				} else if sqlCollector.GetPairCount() > 0 && s.Logger != nil {
					s.Logger(fmt.Sprintf("[SQL-COLLECTOR] Saved %d SQL pairs to file", sqlCollector.GetPairCount()))
				}
			}
			
			// Save execution recorder data
			if executionRecorder != nil {
				if err := executionRecorder.SaveToFile(); err != nil {
					if s.Logger != nil {
						s.Logger(fmt.Sprintf("[EXECUTION-RECORDER] Failed to save: %v", err))
					}
				} else if executionRecorder.GetRecordCount() > 0 && s.Logger != nil {
					s.Logger(fmt.Sprintf("[EXECUTION-RECORDER] Saved %d execution records to file", executionRecorder.GetRecordCount()))
				}
			}
		}
	}()

	// Helper to emit progress
	emitProgress := func(stage string, progress int, message string, step, total int) {
		if onProgress != nil {
			onProgress(NewProgressUpdate(stage, progress, message, step, total))
		}
	}

	emitProgress(StageInitializing, 5, "progress.initializing_tools", 1, 6)

	// Check for template match first (faster path)
	if len(history) > 0 {
		lastMsg := history[len(history)-1]
		if lastMsg.Role == schema.User {
			if template := templates.DetectTemplate(lastMsg.Content); template != nil {
				if s.Logger != nil {
					s.Logger(fmt.Sprintf("[TEMPLATE] Detected template: %s", template.Name()))
				}

				// Create executor for template
				executor := &templates.ServiceExecutor{
					SQLExecutor: func(ctx context.Context, dsID, query string) ([]map[string]interface{}, error) {
						return s.dsService.ExecuteSQL(dsID, query)
					},
					PythonExecutor: func(code, workDir string) (string, error) {
						if s.pythonPool != nil {
							return s.pythonPool.Execute(code, workDir)
						}
						// Fallback to service
						ps := &PythonService{}
						return ps.ExecuteScript(s.cfg.PythonPath, code)
					},
					SchemaGetter: func(dsID string) ([]templates.TableInfo, error) {
						tables, err := s.dsService.GetDataSourceTables(dsID)
						if err != nil {
							return nil, err
						}
						var result []templates.TableInfo
						for _, tableName := range tables {
							cols, _ := s.dsService.GetDataSourceTableColumns(dsID, tableName)
							result = append(result, templates.TableInfo{
								Name:    tableName,
								Columns: cols,
							})
						}
						return result, nil
					},
				}

				// Template progress callback
				templateProgress := func(stage string, progress int, message string, step, total int) {
					emitProgress(stage, progress, message, step, total)
				}

				result, err := template.Execute(ctx, executor, dataSourceID, templateProgress)
				if err == nil && result.Success {
					emitProgress(StageComplete, 100, "progress.analysis_complete", 6, 6)
					if s.Logger != nil {
						s.Logger(fmt.Sprintf("[TIMING] Template execution took: %v", time.Since(startTotal)))
					}
					return &schema.Message{
						Role:    schema.Assistant,
						Content: result.Output,
					}, nil
				}
				// If template failed, fall through to normal LLM flow
				if s.Logger != nil {
					s.Logger(fmt.Sprintf("[TEMPLATE] Template failed, falling back to LLM: %v", err))
				}
			}
		}
	}

	// ğŸš€ Unified Python Analysis Path: Single LLM call for data analysis
	// This path consolidates multiple LLM calls into one for better performance
	if dataSourceID != "" && len(history) > 0 {
		lastMsg := history[len(history)-1]
		if lastMsg.Role == schema.User {
			userQuery := lastMsg.Content
			
			// Use LLM-based classification for more accurate routing
			router := NewRequestRouterWithLLM(s.ChatModel, s.Logger)
			path, classificationResult := router.RouteRequestWithLLM(ctx, userQuery, dataSourceID)
			
			// Log classification result
			if classificationResult != nil && s.Logger != nil {
				s.Logger(fmt.Sprintf("[CLASSIFIER] LLM result: type=%s, viz=%v, export=%v, confidence=%.2f",
					classificationResult.RequestType, 
					classificationResult.NeedsVisualization,
					classificationResult.NeedsDataExport,
					classificationResult.Confidence))
			}
			
			if path == PathUnified {
				if s.Logger != nil {
					s.Logger("[UNIFIED] Attempting unified Python analysis path")
				}
				
				// Get database path for the data source
				var dbPath string
				if sources, err := s.dsService.LoadDataSources(); err == nil {
					for _, ds := range sources {
						if ds.ID == dataSourceID {
							dbPath = ds.Config.DBPath
							break
						}
					}
				}
				
				if dbPath != "" && sessionDir != "" {
					// Create metrics collector
					metrics := NewAnalysisMetrics(s.Logger)
					
					// Create unified generator
					generator := NewUnifiedPythonGenerator(s.ChatModel, s.dsService, s.Logger)
					generator.SetMetrics(metrics)
					
					// Pass classification result to generator for better code generation
					if classificationResult != nil {
						generator.SetClassificationHints(classificationResult)
					}
					
					// Generate complete Python code in single LLM call
					emitProgress(StageAnalysis, 30, "progress.generating_code", 2, 6)
					generatedCode, err := generator.GenerateAnalysisCode(ctx, userQuery, dataSourceID, dbPath, sessionDir)
					
					if err == nil && generatedCode != nil && generatedCode.Code != "" {
						if s.Logger != nil {
							s.Logger(fmt.Sprintf("[UNIFIED] Code generated successfully, %d SQL queries detected", len(generatedCode.SQLQueries)))
						}
						
						// Create execution safety wrapper
						safety := NewExecutionSafety(s.Logger)
						safety.SetTimeout(120 * time.Second) // 2 minute timeout
						
						// Generate safety report
						safetyReport := safety.GenerateSafetyReport(generatedCode.Code)
						if !safetyReport.IsSafe {
							if s.Logger != nil {
								s.Logger(fmt.Sprintf("[UNIFIED] Code blocked by safety check: %v", safetyReport.Errors))
							}
							// Fall through to multi-step path
						} else {
							// Log any warnings
							for _, warning := range safetyReport.Warnings {
								if s.Logger != nil {
									s.Logger(fmt.Sprintf("[UNIFIED] Safety warning: %s", warning))
								}
							}
							
							// Execute the generated Python code with safety wrapper
							emitProgress(StageAnalysis, 60, "progress.running_python", 4, 6)
							
							execStart := time.Now()
							safeResult := safety.ValidateAndExecute(ctx, generatedCode.Code, func(code string) (string, error) {
								if s.pythonPool != nil {
									return s.pythonPool.Execute(code, sessionDir)
								}
								ps := &PythonService{}
								return ps.ExecuteScript(s.cfg.PythonPath, code)
							})
							metrics.RecordExecution(time.Since(execStart))
							
							if safeResult.TimedOut {
								if s.Logger != nil {
									s.Logger(fmt.Sprintf("[UNIFIED] Execution timed out after %v, falling back to multi-step", safeResult.Duration))
								}
								// Fall through to multi-step path
							} else if safeResult.Blocked {
								if s.Logger != nil {
									s.Logger(fmt.Sprintf("[UNIFIED] Execution blocked: %s", safeResult.BlockReason))
								}
								// Fall through to multi-step path
							} else if safeResult.Success {
								// Parse the execution result
								parser := NewResultParser(s.Logger)
								parsedResult := parser.ParseOutput(safeResult.Output, sessionDir)
								
								// Emit file events for generated files
								if onFileSaved != nil {
									for _, f := range parsedResult.ChartFiles {
										onFileSaved(f.Name, f.Type, f.Size)
									}
									for _, f := range parsedResult.ExportFiles {
										onFileSaved(f.Name, f.Type, f.Size)
									}
								}
								
								// Log metrics summary
								metrics.LogSummary()
								
								emitProgress(StageComplete, 100, "progress.analysis_complete", 6, 6)
								trajectory.Success = true
								trajectory.FinalResponse = safeResult.Output
								trajectory.IterationCount = 1
								trajectory.ToolCallCount = 2 // Schema fetch + Python execution
								
								if s.Logger != nil {
									s.Logger(fmt.Sprintf("[TIMING] Unified analysis path took: %v", time.Since(startTotal)))
								}
								
								return &schema.Message{
									Role:    schema.Assistant,
									Content: parser.FormatAsText(parsedResult),
								}, nil
							} else {
								// Execution failed
								if s.Logger != nil {
									s.Logger(fmt.Sprintf("[UNIFIED] Execution failed: %v, falling back to multi-step", safeResult.Error))
								}
							}
						}
					} else if s.Logger != nil {
						s.Logger(fmt.Sprintf("[UNIFIED] Code generation failed: %v, falling back to multi-step", err))
					}
				}
			} else if s.Logger != nil {
				s.Logger(fmt.Sprintf("[UNIFIED] Request routed to %s path, skipping unified", router.GetPathDescription(path)))
			}
		}
	}

	// ğŸ¯ Analysis Planner: Create execution plan before running
	var planPrompt string
	if len(history) > 0 {
		// Extract user query
		var userQuery string
		for i := len(history) - 1; i >= 0; i-- {
			if history[i].Role == schema.User {
				userQuery = history[i].Content
				break
			}
		}

		if userQuery != "" {
			// Create planner and generate plan
			planner := NewAnalysisPlanner(s.ChatModel, s.Logger)
			
			// Get data source info for planning
			dataSourceInfo := "æ— æ•°æ®æº"
			if dataSourceID != "" {
				if sources, err := s.dsService.LoadDataSources(); err == nil {
					for _, ds := range sources {
						if ds.ID == dataSourceID {
							tables, _ := s.dsService.GetDataSourceTables(dataSourceID)
							dataSourceInfo = fmt.Sprintf("æ•°æ®æº: %s, è¡¨: %s", ds.Name, strings.Join(tables, ", "))
							break
						}
					}
				}
			}

			// Generate execution plan
			plan, err := planner.PlanAnalysis(ctx, userQuery, dataSourceInfo)
			if err == nil && plan != nil {
				planPrompt = planner.FormatPlanForPrompt(plan)
				
				// For quick path tasks, execute directly without full agent loop
				if plan.IsQuickPath && plan.QuickPathCode != "" {
					if s.Logger != nil {
						s.Logger("[PLANNER] Executing quick path directly")
					}
					
					// Execute Python code directly
					var result string
					var execErr error
					if s.pythonPool != nil {
						result, execErr = s.pythonPool.Execute(plan.QuickPathCode, sessionDir)
					} else {
						ps := &PythonService{}
						result, execErr = ps.ExecuteScript(s.cfg.PythonPath, plan.QuickPathCode)
					}
					
					if execErr == nil {
						emitProgress(StageComplete, 100, "progress.analysis_complete", 6, 6)
						trajectory.Success = true
						trajectory.FinalResponse = result
						trajectory.IterationCount = 1
						trajectory.ToolCallCount = 1
						if s.Logger != nil {
							s.Logger(fmt.Sprintf("[TIMING] Quick path execution took: %v", time.Since(startTotal)))
						}
						return &schema.Message{
							Role:    schema.Assistant,
							Content: result,
						}, nil
					}
					// If quick path failed, fall through to normal flow
					if s.Logger != nil {
						s.Logger(fmt.Sprintf("[PLANNER] Quick path failed: %v, falling back to normal flow", execErr))
					}
				}
			}
		}
	}

	// 1. Initialize Tools (parallelized for speed)
	startTools := time.Now()
	
	// Use sync.WaitGroup for parallel tool initialization
	var wg sync.WaitGroup
	var pyTool *PythonExecutorTool
	var dsTool *DataSourceContextTool
	var sqlTool *SQLExecutorTool
	var webSearchTool tool.BaseTool // Changed to interface to support multiple search implementations
	var webFetchTool *WebFetchTool  // HTTP-based web content fetcher (no Chrome dependency)
	var mcpTool *MCPTool
	var exportTool *ExportTool
	
	// Initialize Python tool
	wg.Add(1)
	go func() {
		defer wg.Done()
		if s.pythonPool != nil {
			pyTool = NewPythonExecutorToolWithPool(s.cfg, s.pythonPool)
		} else {
			pyTool = NewPythonExecutorTool(s.cfg)
		}
		pyTool.SetErrorKnowledge(s.errorKnowledge)
		if executionRecorder != nil {
			pyTool.SetExecutionRecorder(executionRecorder)
		}
		if sessionDir != "" {
			pyTool.SetSessionDirectory(sessionDir)
			if userMessageID != "" {
				pyTool.SetRequestID(userMessageID)
			}
			if onFileSaved != nil {
				pyTool.SetFileSavedCallback(onFileSaved)
			}
		}
	}()
	
	// Initialize DataSource tool
	wg.Add(1)
	go func() {
		defer wg.Done()
		dsTool = NewDataSourceContextTool(s.dsService)
		if s.workingContextManager != nil {
			dsTool.SetWorkingContextManager(s.workingContextManager)
		}
		if sqlCollector != nil {
			dsTool.SetSQLCollector(sqlCollector)
		}
	}()
	
	// Initialize SQL tool
	wg.Add(1)
	go func() {
		defer wg.Done()
		sqlPlanner := NewSQLPlanner(s.ChatModel, s.dsService, s.Logger)
		sqlTool = NewSQLExecutorToolWithPlanner(s.dsService, sqlPlanner, s.Logger)
		sqlTool.SetErrorKnowledge(s.errorKnowledge)
		if executionRecorder != nil {
			sqlTool.SetExecutionRecorder(executionRecorder)
		}
		if sqlCollector != nil {
			sqlTool.SetSQLCollector(sqlCollector)
			if len(history) > 0 {
				for i := len(history) - 1; i >= 0; i-- {
					if history[i].Role == schema.User {
						sqlCollector.SetUserRequest(history[i].Content)
						break
					}
				}
			}
		}
	}()
	
	// Initialize Web tools (using new API-based search)
	wg.Add(1)
	go func() {
		defer wg.Done()
		// Initialize search API configuration
		s.cfg.InitializeSearchAPIs()
		activeAPI := s.cfg.GetActiveSearchAPI()
		
		if activeAPI != nil && activeAPI.Enabled {
			searchTool, err := NewSearchAPITool(s.Logger, activeAPI)
			if err != nil {
				if s.Logger != nil {
					s.Logger(fmt.Sprintf("[SEARCH-API] Failed to initialize search tool: %v", err))
				}
				// Fallback to nil - will be handled later
				webSearchTool = nil
			} else {
				webSearchTool = searchTool
				if s.Logger != nil {
					s.Logger(fmt.Sprintf("[SEARCH-API] Initialized %s search API", activeAPI.Name))
				}
			}
		} else {
			if s.Logger != nil {
				s.Logger("[SEARCH-API] No active search API configured")
			}
			webSearchTool = nil
		}
		
		// Initialize HTTP-based web fetch tool (no Chrome dependency)
		webFetchTool = NewWebFetchTool(s.Logger, s.cfg.ProxyConfig)
	}()
	
	// Initialize MCP tool
	wg.Add(1)
	go func() {
		defer wg.Done()
		mcpTool = NewMCPTool(s.cfg.MCPServices, s.Logger)
	}()
	
	// Initialize Export tool
	wg.Add(1)
	go func() {
		defer wg.Done()
		exportTool = NewExportTool(s.Logger)
		if sessionDir != "" {
			exportTool.SetSessionDirectory(sessionDir)
			if userMessageID != "" {
				exportTool.SetRequestID(userMessageID)
			}
			if onFileSaved != nil {
				exportTool.SetFileSavedCallback(onFileSaved)
			}
		}
	}()
	
	// Wait for all tools to initialize
	wg.Wait()
	
	if sessionDir != "" && s.Logger != nil {
		s.Logger(fmt.Sprintf("[SESSION] Files will be saved to: %s", sessionDir))
	}

	// Build tools list - only add search tool if it was successfully initialized
	tools := []tool.BaseTool{pyTool, dsTool, sqlTool, webFetchTool, exportTool}
	
	if webSearchTool != nil {
		tools = append(tools, webSearchTool)
		if s.Logger != nil {
			activeAPI := s.cfg.GetActiveSearchAPI()
			if activeAPI != nil {
				s.Logger(fmt.Sprintf("[SEARCH-API] %s search tool added to agent", activeAPI.Name))
			}
		}
	}
	
	if s.Logger != nil {
		activeAPI := s.cfg.GetActiveSearchAPI()
		apiName := "none"
		if activeAPI != nil {
			apiName = activeAPI.Name
		}
		s.Logger(fmt.Sprintf("[WEB-TOOLS] Web search API: %s, Web fetch: HTTP-based (no Chrome)", apiName))
	}

	// Add MCP tool if services are configured
	if mcpTool.HasServices() {
		tools = append(tools, mcpTool)
		if s.Logger != nil {
			services := mcpTool.GetAvailableServices()
			s.Logger(fmt.Sprintf("[MCP] Loaded %d MCP service(s): %s", 
				len(services), strings.Join(services, ", ")))
		}
	} else {
		if s.Logger != nil {
			s.Logger("[MCP] No MCP services configured or enabled")
		}
	}

	// 2. Create ToolsNode (Standard Eino ToolsNode takes *Message and returns *Message)
	toolsNode, err := compose.NewToolNode(ctx, &compose.ToolsNodeConfig{
		Tools: tools,
	})
	if err != nil {
		return nil, fmt.Errorf("failed to create tools node: %v", err)
	}
	if s.Logger != nil {
		s.Logger(fmt.Sprintf("[TIMING] Tools Initialization took: %v", time.Since(startTools)))
	}

	// 3. Bind Tool Infos to Model
	startBind := time.Now()
	var toolInfos []*schema.ToolInfo
	for _, t := range tools {
		info, err := t.Info(ctx)
		if err != nil {
			return nil, err
		}
		toolInfos = append(toolInfos, info)
	}
	err = s.ChatModel.BindTools(toolInfos)
	if err != nil {
		return nil, fmt.Errorf("failed to bind tools: %v", err)
	}
	if s.Logger != nil {
		s.Logger(fmt.Sprintf("[TIMING] Binding Tools took: %v", time.Since(startBind)))
	}

	emitProgress(StageInitializing, 10, "progress.tools_ready", 1, 6)

	// 4. Build Graph using Lambda nodes to manage state ([]*schema.Message)
	startGraph := time.Now()
	g := compose.NewGraph[[]*schema.Message, []*schema.Message]()

	// Track iteration count for progress
	iterationCount := 0

	// Extract original user goal for attention refresh
	var originalUserGoal string
	for i := len(history) - 1; i >= 0; i-- {
		if history[i].Role == schema.User {
			originalUserGoal = history[i].Content
			if len(originalUserGoal) > 200 {
				originalUserGoal = originalUserGoal[:200] + "..."
			}
			break
		}
	}

	// deduplicateMessages removes duplicate consecutive messages with same role and content
	deduplicateMessages := func(messages []*schema.Message) []*schema.Message {
		if len(messages) <= 1 {
			return messages
		}
		
		result := make([]*schema.Message, 0, len(messages))
		seen := make(map[string]bool)
		duplicateCount := 0
		
		for _, msg := range messages {
			// Create a unique key for this message
			key := fmt.Sprintf("%s:%s", msg.Role, msg.Content)
			
			// For user messages, always check for duplicates
			if msg.Role == schema.User {
				if seen[key] {
					// Skip duplicate user message
					duplicateCount++
					if s.Logger != nil {
						contentPreview := msg.Content
						if len(contentPreview) > 50 {
							contentPreview = contentPreview[:50] + "..."
						}
						s.Logger(fmt.Sprintf("[DEDUP] Filtered duplicate user message: %s", contentPreview))
					}
					continue
				}
				seen[key] = true
			}
			
			result = append(result, msg)
		}
		
		if duplicateCount > 0 && s.Logger != nil {
			s.Logger(fmt.Sprintf("[DEDUP] Removed %d duplicate message(s), %d -> %d messages", 
				duplicateCount, len(messages), len(result)))
		}
		
		return result
	}

	// Define Model Node Wrapper
	modelLambda := compose.InvokableLambda(func(ctx context.Context, input []*schema.Message) ([]*schema.Message, error) {
		iterationCount++
		startModel := time.Now()

		// Check for cancellation
		if cancelCheck != nil && cancelCheck() {
			if s.Logger != nil {
				s.Logger(fmt.Sprintf("[CANCEL] Analysis cancelled at step %d", iterationCount))
			}
			return nil, fmt.Errorf("analysis cancelled by user")
		}

		// ğŸ”´ CRITICAL: Remove duplicate messages before processing
		input = deduplicateMessages(input)

		// âš¡ EARLY WARNINGS: Encourage completion before hitting limits
		// More aggressive warnings to speed up completion
		if iterationCount == 6 {
			warningMsg := &schema.Message{
				Role:    schema.User,
				Content: "âš¡ å·²ç”¨6æ­¥ã€‚å°½å¿«å®Œæˆ,æœ€å¤šå†ç”¨2æ¬¡å·¥å…·ã€‚",
			}
			input = append(input, warningMsg)
			if s.Logger != nil {
				s.Logger("[WARNING] Step 6 warning injected")
			}
		} else if iterationCount == 8 {
			warningMsg := &schema.Message{
				Role:    schema.User,
				Content: "âš ï¸ å·²ç”¨8æ­¥ã€‚ç«‹å³å‘ˆç°ç»“æœ,ä¸è¦å†è°ƒç”¨å·¥å…·ã€‚",
			}
			input = append(input, warningMsg)
			if s.Logger != nil {
				s.Logger("[WARNING] Step 8 warning injected")
			}
		} else if iterationCount == 10 {
			finalMsg := &schema.Message{
				Role:    schema.User,
				Content: "ğŸ›‘ åœæ­¢! ç«‹å³è¾“å‡ºå½“å‰ç»“æœã€‚",
			}
			input = append(input, finalMsg)
			if s.Logger != nil {
				s.Logger("[FINAL-WARNING] Step 10 final warning injected")
			}
		}

		// Emit progress based on iteration
		progress := 20 + min(iterationCount*10, 60) // 20-80%
		emitProgress(StageAnalysis, progress, "progress.ai_processing", 3, 6)

		// Apply memory management only if enabled in config
		managedInput := input
		if s.cfg.EnableMemory && s.memoryManager != nil {
			var err error
			managedInput, err = s.memoryManager.ManageMemory(ctx, input)
			if err != nil {
				if s.Logger != nil {
					s.Logger(fmt.Sprintf("[WARNING] Memory management failed in graph: %v", err))
				}
				managedInput = input
			}

			// Log token reduction if significant
			if s.Logger != nil && len(input) != len(managedInput) {
				originalTokens := s.memoryManager.EstimateTokens(input)
				managedTokens := s.memoryManager.EstimateTokens(managedInput)
				s.Logger(fmt.Sprintf("[MEMORY-GRAPH] Reduced from %d to %d messages (%d -> %d est. tokens)",
					len(input), len(managedInput), originalTokens, managedTokens))
			}
		} else if s.Logger != nil && !s.cfg.EnableMemory {
			s.Logger("[MEMORY] Memory management disabled by config")
		}

		// Call model with managed history
		resp, err := s.ChatModel.Generate(ctx, managedInput)
		if err != nil {
			// Record error in trajectory
			step := TrajectoryStep{
				StepNumber: len(trajectory.Steps) + 1,
				Timestamp:  time.Now().UnixMilli(),
				Type:       "model_call",
				ModelInput: messagesToMap(managedInput),
				Error:      err.Error(),
			}
			trajectory.Steps = append(trajectory.Steps, step)
			return nil, err
		}

		// Record successful model call in trajectory with escaped content
		step := TrajectoryStep{
			StepNumber:  len(trajectory.Steps) + 1,
			Timestamp:   time.Now().UnixMilli(),
			Type:        "model_call",
			ModelInput:  messagesToMap(managedInput),
			ModelOutput: messageToMap(resp),
		}
		trajectory.Steps = append(trajectory.Steps, step)
		trajectory.IterationCount = iterationCount

		if s.Logger != nil {
			s.Logger(fmt.Sprintf("[TIMING] Model Generation step took: %v", time.Since(startModel)))
		}
		// Append response to managed history (use managedInput to avoid duplicates)
		return append(managedInput, resp), nil
	})

	// Define Tool Node Wrapper
	toolsLambda := compose.InvokableLambda(func(ctx context.Context, input []*schema.Message) ([]*schema.Message, error) {
		startExec := time.Now()
		
		// Check for cancellation before executing tools
		if cancelCheck != nil && cancelCheck() {
			if s.Logger != nil {
				s.Logger("[CANCEL] Analysis cancelled before tool execution")
			}
			return nil, fmt.Errorf("analysis cancelled by user")
		}
		
		// Get the last message (which should be Assistant with ToolCalls)
		if len(input) == 0 {
			return nil, fmt.Errorf("tool node received empty history")
		}
		lastMsg := input[len(input)-1]

		// Emit progress based on tool being called
		if len(lastMsg.ToolCalls) > 0 {
			toolName := lastMsg.ToolCalls[0].Function.Name
			var msg string
			switch toolName {
			case "get_data_source_context":
				emitProgress(StageSchema, 25, "progress.loading_schema", 2, 6)
				msg = "è·å–æ¨¡å¼ä¸­"
			case "execute_sql":
				emitProgress(StageQuery, 40, "progress.executing_sql", 4, 6)
				msg = "æ‰§è¡ŒæŸ¥è¯¢ä¸­"
			case "python_executor":
				emitProgress(StageAnalysis, 60, "progress.running_python", 5, 6)
				msg = "åˆ†ææ•°æ®ä¸­"
			default:
				msg = fmt.Sprintf("Running %s", toolName)
			}
			if s.Logger != nil {
				s.Logger(fmt.Sprintf("[PROGRESS] %s", msg))
				}
		}

		// Execute tools
		toolResultMsg, err := toolsNode.Invoke(ctx, lastMsg)

		// Record tool calls in trajectory with escaped content for training visibility
		for _, tc := range lastMsg.ToolCalls {
			step := TrajectoryStep{
				StepNumber: len(trajectory.Steps) + 1,
				Timestamp:  time.Now().UnixMilli(),
				Type:       "tool_call",
				ToolName:   tc.Function.Name,
				ToolInput:  escapeForTraining(tc.Function.Arguments),
				ToolCallID: tc.ID,
			}

			if err != nil {
				step.Error = escapeForTraining(err.Error())
			} else if len(toolResultMsg) > 0 {
				// Find matching tool result for this call - record escaped output for training visibility
				for _, resultMsg := range toolResultMsg {
					if resultMsg.ToolCallID == tc.ID {
						step.ToolOutput = escapeForTraining(resultMsg.Content)
						break
					}
				}
			}

			trajectory.Steps = append(trajectory.Steps, step)
			trajectory.ToolCallCount++
		}

		if err != nil {
			// Instead of failing the graph, return error as tool result so LLM can retry
			if s.Logger != nil {
				s.Logger(fmt.Sprintf("[TOOL ERROR] %v - returning as message for LLM to handle", err))
			}
			// Create error messages for each tool call with helpful guidance
			var errorMsgs []*schema.Message
			errStr := err.Error()
			for _, tc := range lastMsg.ToolCalls {
				var helpMsg string
				toolName := tc.Function.Name

				if toolName == "execute_sql" {
					if strings.Contains(errStr, "no such column") || strings.Contains(errStr, "Unknown column") {
						helpMsg = fmt.Sprintf("âŒ SQL Column Error: %v\n\n", err)
						helpMsg += "ğŸ”§ REQUIRED ACTION:\n"
						helpMsg += "1. Call get_data_source_context to see actual column names\n"
						helpMsg += "2. If using subquery, ensure ALL columns needed by outer query are in subquery's SELECT\n"
						helpMsg += "3. Rewrite and execute the corrected query"
					} else if strings.Contains(errStr, "syntax error") {
						helpMsg = fmt.Sprintf("âŒ SQL Syntax Error: %v\n\n", err)
						helpMsg += "ğŸ”§ For SQLite, use: strftime('%Y',col) not YEAR(), col1||col2 not CONCAT()"
					} else {
						helpMsg = fmt.Sprintf("âŒ SQL Error: %v\n\nğŸ”§ Please fix and retry.", err)
					}
				} else if toolName == "python_executor" {
					helpMsg = fmt.Sprintf("âŒ Python Error: %v\n\nğŸ”§ Please fix the code and retry.", err)
				} else {
					helpMsg = fmt.Sprintf("âŒ Tool Error: %v\n\nğŸ”§ Please fix and retry.", err)
				}

				errorMsgs = append(errorMsgs, &schema.Message{
					Role:       schema.Tool,
					Content:    helpMsg,
					ToolCallID: tc.ID,
				})
			}
			if len(errorMsgs) == 0 {
				// Fallback if no tool calls found
				errorMsgs = append(errorMsgs, &schema.Message{
					Role:    schema.Tool,
					Content: fmt.Sprintf("Error: %v\n\nğŸ”´ Please fix the issue and try again.", err),
				})
			}
			return append(input, errorMsgs...), nil
		}
		if s.Logger != nil {
			s.Logger(fmt.Sprintf("[TIMING] Tools Execution step took: %v", time.Since(startExec)))
		}

		// CRITICAL: Truncate tool output to prevent context overflow
		// Tool outputs (especially SQL results) can be huge
		const maxToolOutputChars = 50000 // Very high limit to prevent truncation of important data
		for i, msg := range toolResultMsg {
			if msg.Role == schema.Tool && len(msg.Content) > maxToolOutputChars {
				toolResultMsg[i] = &schema.Message{
					Role:       msg.Role,
					Content:    msg.Content[:maxToolOutputChars] + fmt.Sprintf("\n\n[... Output truncated - %d chars omitted for context limit]", len(msg.Content)-maxToolOutputChars),
					ToolCallID: msg.ToolCallID,
				}
			}
		}

		// Stream tool output to frontend
		if len(toolResultMsg) > 0 && onProgress != nil {
			for _, msg := range toolResultMsg {
				if msg.Role == schema.Tool && msg.Content != "" {
					// Get tool name from the original call
					toolName := ""
					if len(lastMsg.ToolCalls) > 0 {
						toolName = lastMsg.ToolCalls[0].Function.Name
					}

					// Truncate output for streaming preview (keep full in final response)
					preview := msg.Content
					if len(preview) > 200 {
						preview = preview[:200] + "..."
					}

					onProgress(ProgressUpdate{
						Stage:      "tool_output",
						Progress:   65,
						Message:    "progress.tool_completed",
						Step:       4,
						Total:      6,
						ToolName:   toolName,
						ToolOutput: preview,
					})
				}
			}
		}

		// Append tool result to history
		return append(input, toolResultMsg...), nil
	})

	err = g.AddLambdaNode("model", modelLambda)
	if err != nil {
		return nil, err
	}

	err = g.AddLambdaNode("tools", toolsLambda)
	if err != nil {
		return nil, err
	}

	err = g.AddEdge(compose.START, "model")
	if err != nil {
		return nil, err
	}

	// Branch: loop back to tools or end
	err = g.AddBranch("model", compose.NewGraphBranch(func(ctx context.Context, history []*schema.Message) (string, error) {
		lastMsg := history[len(history)-1]
		if len(lastMsg.ToolCalls) > 0 {
			return "tools", nil
		}
		return compose.END, nil
	}, map[string]bool{"tools": true, compose.END: true}))
	if err != nil {
		return nil, err
	}

	err = g.AddEdge("tools", "model")
	if err != nil {
		return nil, err
	}

	// 5. Compile and Run with reduced max steps for better efficiency
	runnable, err := g.Compile(ctx, compose.WithMaxRunSteps(20))
	if err != nil {
		return nil, fmt.Errorf("failed to compile graph: %v", err)
	}
	if s.Logger != nil {
		s.Logger(fmt.Sprintf("[TIMING] Graph Construction & Compilation took: %v", time.Since(startGraph)))
	}

	emitProgress(StageInitializing, 15, "Preparing context...", 1, 6)

	// 6. Build Context Prompt (minimal - only table names, let tool provide details)
	startContext := time.Now()
	var contextPrompt string
	var dbType string = "sqlite"
	if dataSourceID != "" && s.dsService != nil {
		sources, _ := s.dsService.LoadDataSources()
		for _, ds := range sources {
			if ds.ID == dataSourceID {
				// Determine database type
				if ds.Config.DBPath != "" {
					dbType = "sqlite"
				} else if ds.Type == "mysql" || ds.Type == "doris" {
					dbType = ds.Type
				}

				contextPrompt = fmt.Sprintf("\n\nData: %s (ID: %s, Type: %s)\n", ds.Name, ds.ID, strings.ToUpper(dbType))
				if ds.Analysis != nil && ds.Analysis.Summary != "" {
					contextPrompt += fmt.Sprintf("Summary: %s\n", ds.Analysis.Summary)
				}

				// Only send table names, not full schema
				if ds.Analysis != nil && len(ds.Analysis.Schema) > 0 {
					var tableNames []string
					for _, t := range ds.Analysis.Schema {
						tableNames = append(tableNames, t.TableName)
					}
					contextPrompt += fmt.Sprintf("Tables: %s\n", strings.Join(tableNames, ", "))
					contextPrompt += "âš ï¸ Call get_data_source_context for columns.\n"
				}

				// SQL dialect
				if dbType == "sqlite" {
					contextPrompt += `Dialect: SQLite (use strftime, ||, no YEAR/MONTH)`
				} else if dbType == "mysql" || dbType == "doris" {
					contextPrompt += `Dialect: MySQL (use YEAR/MONTH, CONCAT)`
				}
				break
			}
		}
	}
	if s.Logger != nil {
		s.Logger(fmt.Sprintf("[TIMING] Context Prompt preparation took: %v", time.Since(startContext)))
	}

	// Load working context if available for context-aware analysis
	var workingContextPrompt string
	if threadID != "" && s.workingContextManager != nil {
		if ctx := s.workingContextManager.GetContext(threadID); ctx != nil {
			workingContextPrompt = ctx.FormatForPrompt()
			if s.Logger != nil {
				s.Logger("[WORKING-CONTEXT] Loaded context for prompt injection")
			}
		}
	}

	// Load conversation context for better follow-up understanding
	var conversationContextPrompt string
	if threadID != "" && s.conversationContextManager != nil {
		conversationContextPrompt = s.conversationContextManager.GetContextForPrompt(threadID)
		if conversationContextPrompt != "" && s.Logger != nil {
			s.Logger("[CONVERSATION-CONTEXT] Loaded conversation context for prompt injection")
		}
	}

	// Build MCP tools prompt if services are available
	var mcpToolsPrompt string
	if len(s.cfg.MCPServices) > 0 {
		// Filter enabled and tested services
		var availableServices []string
		for _, svc := range s.cfg.MCPServices {
			if svc.Enabled && svc.Tested {
				availableServices = append(availableServices, 
					fmt.Sprintf("  â€¢ %s: %s", svc.Name, svc.Description))
			}
		}
		
		if len(availableServices) > 0 {
			mcpToolsPrompt = "\n\nğŸ”Œ MCP SERVICES (External capabilities):\n"
			mcpToolsPrompt += strings.Join(availableServices, "\n")
			mcpToolsPrompt += "\n- Use mcp_service tool to call these services"
			mcpToolsPrompt += "\n- Specify service_name, method (GET/POST), and endpoint"
			mcpToolsPrompt += "\n- Useful for accessing external APIs and real-time data"
			
			if s.Logger != nil {
				s.Logger(fmt.Sprintf("[MCP-PROMPT] Added %d MCP service(s) to system prompt", len(availableServices)))
			}
		}
	}

	// Add analysis plan to prompt if available
	analysisPlanPrompt := ""
	if planPrompt != "" {
		analysisPlanPrompt = planPrompt
	}

	sysMsg := &schema.Message{
		Role:    schema.System,
		Content: `RapidBIæ•°æ®åˆ†æä¸“å®¶ã€‚å¿«é€Ÿã€ç›´æ¥ã€å¯è§†åŒ–ä¼˜å…ˆã€‚

ğŸ¯ ç›®æ ‡: é«˜è´¨é‡åˆ†æäº§å‡ºï¼ˆå›¾è¡¨+æ•°æ®+æ´å¯Ÿï¼‰

ğŸ“Š **å¯è§†åŒ–æ–¹å¼ï¼ˆäºŒé€‰ä¸€ï¼‰**:

**æ–¹å¼1: EChartsï¼ˆæ¨èï¼Œæ— éœ€æ‰§è¡Œä»£ç ï¼‰**
- ç›´æ¥åœ¨å›å¤ä¸­è¾“å‡º ` + "```json:echarts\n{...}\n```" + `
- å‰ç«¯ä¼šè‡ªåŠ¨æ¸²æŸ“å›¾è¡¨
- é€‚åˆï¼šäº¤äº’å¼å›¾è¡¨ã€å¿«é€Ÿå±•ç¤º
- ğŸš« **EChartsç»å¯¹ä¸ä¼šç”Ÿæˆä»»ä½•æ–‡ä»¶ï¼** ä¸è¦è¯´"å·²ç”Ÿæˆxxx.pdf"æˆ–"å·²ä¿å­˜xxx.png"

**æ–¹å¼2: Python matplotlibï¼ˆéœ€è¦æ‰§è¡Œä»£ç æ‰èƒ½ç”Ÿæˆæ–‡ä»¶ï¼‰**
- å¿…é¡»è°ƒç”¨python_executorå·¥å…·æ‰§è¡Œä»£ç 
- ä½¿ç”¨FILES_DIRå˜é‡ä¿å­˜æ–‡ä»¶
- é€‚åˆï¼šéœ€è¦å¯¼å‡ºPDF/PNGæ–‡ä»¶æ—¶
- âœ… åªæœ‰python_executoræ‰§è¡ŒæˆåŠŸåï¼Œæ–‡ä»¶æ‰çœŸæ­£å­˜åœ¨

ğŸš¨ğŸš¨ğŸš¨ **ä¸¥ç¦è™šå‡æ–‡ä»¶å£°æ˜ï¼ˆæœ€é‡è¦è§„åˆ™ï¼‰** ğŸš¨ğŸš¨ğŸš¨
- **ECharts = å‰ç«¯æ¸²æŸ“ = æ— æ–‡ä»¶ç”Ÿæˆ** â†’ ç»å¯¹ä¸èƒ½è¯´"å›¾è¡¨å·²ç”Ÿæˆ: xxx.pdf"
- **åªæœ‰è°ƒç”¨python_executorå¹¶æ‰§è¡ŒæˆåŠŸåï¼Œæ‰èƒ½å£°ç§°æ–‡ä»¶å·²ç”Ÿæˆ**
- **è¿è§„ç¤ºä¾‹ï¼ˆç»å¯¹ç¦æ­¢ï¼‰**:
  - âŒ "å›¾è¡¨æ–‡ä»¶å·²ç”Ÿæˆ: analysis.pdf (32KB)" â† å¦‚æœæ²¡è°ƒç”¨python_executorï¼Œè¿™æ˜¯è™šå‡å£°æ˜
  - âŒ "âœ… æ•£ç‚¹å›¾: scatter.pdf (28KB)" â† å¦‚æœåªç”¨äº†EChartsï¼Œè¿™æ˜¯è™šå‡å£°æ˜
- **æ­£ç¡®ç¤ºä¾‹**:
  - âœ… ä½¿ç”¨EChartsæ—¶: "ä»¥ä¸‹æ˜¯äº¤äº’å¼å›¾è¡¨:" + json:echartsä»£ç å—ï¼ˆä¸æåŠä»»ä½•æ–‡ä»¶ï¼‰
  - âœ… ä½¿ç”¨matplotlibæ—¶: å…ˆè°ƒç”¨python_executorï¼Œæ‰§è¡ŒæˆåŠŸåæ‰è¯´"æ–‡ä»¶å·²ä¿å­˜"

âš¡ å¿«é€Ÿè·¯å¾„(è·³è¿‡æœç´¢,ç›´æ¥ç”¨python_executor):
- æ—¶é—´/æ—¥æœŸæŸ¥è¯¢ â†’ datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
- æ•°å­¦è®¡ç®— â†’ ç›´æ¥è®¡ç®—
- å•ä½æ¢ç®— â†’ ç›´æ¥æ¢ç®—

ğŸ“‹ æ•°æ®åˆ†ææ ‡å‡†æµç¨‹:
1. get_data_source_context â†’ è·å–schema
2. execute_sql â†’ æŸ¥è¯¢æ•°æ®
3. å¯è§†åŒ–ï¼šECharts(ç›´æ¥è¾“å‡º,æ— æ–‡ä»¶) æˆ– python_executor(ç”Ÿæˆæ–‡ä»¶)
4. å‘ˆç°ç»“æœ(å›¾è¡¨+æ´å¯Ÿ+æ•°æ®è¡¨)

ğŸ“¤ æ•°æ®å¯¼å‡ºè§„åˆ™:
- â­ æ•°æ®è¡¨æ ¼å¯¼å‡º â†’ Excelæ ¼å¼(export_data, format="excel")
- å¯è§†åŒ–æŠ¥å‘Š â†’ PDFæ ¼å¼(éœ€è¦python_executor)
- æ¼”ç¤ºæ–‡ç¨¿ â†’ PPTæ ¼å¼

ğŸ”´ å…³é”®è§„åˆ™:
- **åˆ†æè¯·æ±‚å¿…é¡»æœ‰å¯è§†åŒ–** - EChartsæˆ–matplotlib
- **EChartsä¸ç”Ÿæˆæ–‡ä»¶ï¼Œä¸è¦å£°ç§°ç”Ÿæˆäº†æ–‡ä»¶**
- ç«‹å³æ‰§è¡Œå·¥å…·(ä¸è¦å…ˆè§£é‡Š)
- get_data_source_contextæœ€å¤šè°ƒç”¨2æ¬¡
- SQLé”™è¯¯æ—¶ç›´æ¥ä¿®å¤

ğŸ“Š è¾“å‡ºæ ¼å¼:
- EChartså›¾è¡¨: ` + "```json:echarts\n{...}\n```" + ` (ä»…å‰ç«¯æ¸²æŸ“ï¼Œæ— æ–‡ä»¶)
- è¡¨æ ¼: ` + "```json:table\n[...]\n```" + `
- å›¾ç‰‡ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ˜¾ç¤º

ğŸŒ ç½‘ç»œæœç´¢(ä»…ç”¨äºå¤–éƒ¨ä¿¡æ¯):
- web_search: æ–°é—»ã€è‚¡ä»·ã€å¤©æ°”ç­‰å®æ—¶å¤–éƒ¨æ•°æ®
- web_fetch: è·å–ç½‘é¡µå†…å®¹
- âš ï¸ ä¸è¦ç”¨æœç´¢æŸ¥æ—¶é—´/è®¡ç®—/æœ¬åœ°å¯å®Œæˆçš„ä»»åŠ¡
- å¼•ç”¨æ¥æº: [æ¥æº: URL]

ğŸ‡¨ğŸ‡³ è¯­è¨€: å›¾è¡¨æ ‡é¢˜/æ ‡ç­¾å¿…é¡»ç”¨ä¸­æ–‡

ğŸ“ˆ åˆ†æäº§å‡ºè¦æ±‚:
- æ•°æ®åˆ†æ â†’ å¿…é¡»åŒ…å«: å›¾è¡¨(EChartsæˆ–matplotlib) + å…³é”®æ´å¯Ÿ + æ•°æ®æ‘˜è¦
- ç®€å•é—®é¢˜(æ—¶é—´/è®¡ç®—) â†’ ç›´æ¥è¿”å›ç»“æœ
- ä¸è¦åªè¿”å›çº¯æ–‡å­—åˆ†æï¼Œè¦æœ‰å¯è§†åŒ–æ”¯æ’‘

âš ï¸ é«˜æ•ˆæ‰§è¡Œï¼Œä½†ä¸è¦ç‰ºç‰²åˆ†æè´¨é‡!` + analysisPlanPrompt + contextPrompt + workingContextPrompt + conversationContextPrompt + mcpToolsPrompt,
	}

	// 7. Apply memory management to history (only if enabled)
	startMemory := time.Now()
	managedHistory := history
	if s.cfg.EnableMemory && s.memoryManager != nil {
		var err error
		managedHistory, err = s.memoryManager.ManageMemory(ctx, history)
		if err != nil {
			if s.Logger != nil {
				s.Logger(fmt.Sprintf("[WARNING] Memory management failed: %v, using original history", err))
			}
			managedHistory = history
		}
		if s.Logger != nil {
			originalTokens := s.memoryManager.EstimateTokens(history)
			managedTokens := s.memoryManager.EstimateTokens(managedHistory)
			s.Logger(fmt.Sprintf("[MEMORY] Original: %d msgs (%d est. tokens) -> Managed: %d msgs (%d est. tokens)",
				len(history), originalTokens, len(managedHistory), managedTokens))
			s.Logger(fmt.Sprintf("[TIMING] Memory Management took: %v", time.Since(startMemory)))
		}
	} else if s.Logger != nil {
		s.Logger("[MEMORY] Memory management disabled - using raw history")
	}

	input := append([]*schema.Message{sysMsg}, managedHistory...)

	emitProgress(StageAnalysis, 20, "å¼€å§‹åˆ†æ...", 3, 6)

	startInvoke := time.Now()
	finalHistory, err := runnable.Invoke(ctx, input)
	if err != nil {
		// Mark trajectory as failed
		trajectory.Success = false
		trajectory.ErrorMessage = err.Error()
		return nil, err
	}
	if s.Logger != nil {
		s.Logger(fmt.Sprintf("[TIMING] Graph Execution (Invoke) took: %v", time.Since(startInvoke)))
		s.Logger(fmt.Sprintf("[TIMING] Total RunAnalysis took: %v", time.Since(startTotal)))
	}

	emitProgress(StageComplete, 100, "åˆ†æå®Œæˆ", 6, 6)

	// Return the last message and mark trajectory as successful with escaped content
	if len(finalHistory) > 0 {
		lastMsg := finalHistory[len(finalHistory)-1]
		trajectory.Success = true
		trajectory.FinalResponse = escapeForTraining(lastMsg.Content) // Escape for training visibility
		
		// Update conversation context with assistant response
		if s.conversationContextManager != nil && threadID != "" && lastMsg.Role == schema.Assistant {
			// Extract tool used from history
			var lastToolUsed string
			var lastToolResult string
			for i := len(finalHistory) - 1; i >= 0; i-- {
				if finalHistory[i].Role == schema.Assistant && len(finalHistory[i].ToolCalls) > 0 {
					lastToolUsed = finalHistory[i].ToolCalls[0].Function.Name
					break
				}
				if finalHistory[i].Role == schema.Tool {
					lastToolResult = finalHistory[i].Content
				}
			}
			s.conversationContextManager.UpdateFromAssistantResponse(threadID, lastMsg.Content, lastToolUsed, lastToolResult)
			if s.Logger != nil {
				s.Logger("[CONTEXT] Updated conversation context with assistant response")
			}
		}
		
		// Extract and store valuable memories (only if memory is enabled and analysis was successful)
		// Run asynchronously to not block user response
		if s.cfg.EnableMemory && lastMsg.Role == schema.Assistant && lastMsg.Content != "" {
			go func() {
				startMemoryExtraction := time.Now()
				
				// Collect SQL queries and results from history
				var sqlQueries []string
				var dataResults []map[string]interface{}
				var userQuery string
				
				// Extract user query from history
				for i := len(finalHistory) - 1; i >= 0; i-- {
					if finalHistory[i].Role == schema.User {
						userQuery = finalHistory[i].Content
						break
					}
				}
				
				// Extract SQL queries and results from tool calls
				for i, msg := range finalHistory {
					if msg.Role == schema.Assistant && len(msg.ToolCalls) > 0 {
						for _, tc := range msg.ToolCalls {
							if tc.Function.Name == "execute_sql" {
								// Parse arguments to get SQL query
								var args map[string]interface{}
								if err := json.Unmarshal([]byte(tc.Function.Arguments), &args); err == nil {
									if query, ok := args["query"].(string); ok {
										sqlQueries = append(sqlQueries, query)
									}
								}
								
								// Look for corresponding tool result in next messages
								for j := i + 1; j < len(finalHistory); j++ {
									if finalHistory[j].Role == schema.Tool && finalHistory[j].ToolCallID == tc.ID {
										// Try to parse tool result as data
										var result []map[string]interface{}
										if err := json.Unmarshal([]byte(finalHistory[j].Content), &result); err == nil {
											dataResults = append(dataResults, result...)
										}
										break
									}
								}
							}
						}
					}
				}
				
				// Create memory extractor and extract key findings
				if len(sqlQueries) > 0 || userQuery != "" {
					extractor := NewMemoryExtractor(s.ChatModel, s.Logger)
					memories := extractor.ExtractKeyFindings(
						context.Background(), // Use background context for async operation
						userQuery,
						lastMsg.Content,
						sqlQueries,
						dataResults,
					)
					
					if s.Logger != nil && len(memories) > 0 {
						s.Logger(fmt.Sprintf("[MEMORY] Extracted %d valuable memories from analysis", len(memories)))
					}
					
					// Store memories using MemoryService based on tier
					if s.memoryService != nil {
						for _, mem := range memories {
							var err error
							
							// Route to appropriate memory tier
							switch mem.Tier {
							case LongTermTier:
								// Long-term: persistent facts (schemas, rules, data characteristics)
								err = s.memoryService.AddSessionLongTermMemory(threadID, mem.Content)
								if err != nil && s.Logger != nil {
									s.Logger(fmt.Sprintf("[MEMORY] Failed to store long-term memory: %v", err))
								}
							case MidTermTier:
								// Mid-term: compressed summaries (not used here, managed by MemoryManager)
								err = s.memoryService.AddSessionMediumTermMemory(threadID, mem.Content)
								if err != nil && s.Logger != nil {
									s.Logger(fmt.Sprintf("[MEMORY] Failed to store mid-term memory: %v", err))
								}
							case ShortTermTier:
								// Short-term: current context (not persisted, managed by MemoryManager)
								// Skip persistence for short-term memories
								continue
							}
							
							if err == nil && s.Logger != nil {
								s.Logger(fmt.Sprintf("[MEMORY] âœ“ Stored [%s] %s: %s", 
									mem.Tier,
									mem.Category,
									mem.Content))
							}
						}
					} else if s.Logger != nil {
						// Log only if memoryService is not available
						for _, mem := range memories {
							s.Logger(fmt.Sprintf("[MEMORY] [%s] %s: %s", 
								mem.Tier,
								mem.Category,
								mem.Content))
						}
					}
					
					if s.Logger != nil {
						s.Logger(fmt.Sprintf("[TIMING] Memory extraction took: %v (async)", time.Since(startMemoryExtraction)))
					}
				}
			}()
		}
		
		return lastMsg, nil
	}

	// No response - mark as failed
	trajectory.Success = false
	trajectory.ErrorMessage = "agent returned empty history"
	return nil, fmt.Errorf("agent returned empty history")
}

// saveTrajectory saves the trajectory to session directory for training use
func (s *EinoService) saveTrajectory(sessionDir string, trajectory *AgentTrajectory) {
	if sessionDir == "" || trajectory == nil {
		return
	}

	// Finalize trajectory
	trajectory.EndTime = time.Now().UnixMilli()
	trajectory.TotalDuration = trajectory.EndTime - trajectory.StartTime

	// Create trajectory directory
	trajectoryDir := filepath.Join(sessionDir, "trajectory")
	if err := os.MkdirAll(trajectoryDir, 0755); err != nil {
		if s.Logger != nil {
			s.Logger(fmt.Sprintf("[TRAJECTORY] Failed to create directory: %v", err))
		}
		return
	}

	// Generate filename based on timestamp
	filename := fmt.Sprintf("%d.json", trajectory.StartTime)
	filePath := filepath.Join(trajectoryDir, filename)

	// Create JSON encoder with proper settings for complete data preservation
	file, err := os.Create(filePath)
	if err != nil {
		if s.Logger != nil {
			s.Logger(fmt.Sprintf("[TRAJECTORY] Failed to create file: %v", err))
		}
		return
	}
	defer file.Close()

	encoder := json.NewEncoder(file)
	encoder.SetIndent("", "  ")
	encoder.SetEscapeHTML(false) // Preserve HTML characters in content

	// Encode trajectory to JSON with proper escaping
	if err := encoder.Encode(trajectory); err != nil {
		if s.Logger != nil {
			s.Logger(fmt.Sprintf("[TRAJECTORY] Failed to encode JSON: %v", err))
		}
		return
	}

	if s.Logger != nil {
		s.Logger(fmt.Sprintf("[TRAJECTORY] Saved to: %s (%d steps, %d tool calls, %dms)",
			filePath, len(trajectory.Steps), trajectory.ToolCallCount, trajectory.TotalDuration))
		
		// Verify JSON format by attempting to read it back
		if err := s.verifyTrajectoryJSON(filePath); err != nil {
			s.Logger(fmt.Sprintf("[TRAJECTORY] JSON format verification failed: %v", err))
		} else {
			s.Logger("[TRAJECTORY] JSON format verified successfully")
		}
	}
}

// verifyTrajectoryJSON verifies that the saved trajectory file is valid JSON
func (s *EinoService) verifyTrajectoryJSON(filePath string) error {
	file, err := os.Open(filePath)
	if err != nil {
		return fmt.Errorf("failed to open file: %v", err)
	}
	defer file.Close()

	decoder := json.NewDecoder(file)
	var trajectory AgentTrajectory
	if err := decoder.Decode(&trajectory); err != nil {
		return fmt.Errorf("JSON decode failed: %v", err)
	}

	// Additional verification: check if final_response can be extracted
	if trajectory.FinalResponse != "" {
		if s.Logger != nil {
			s.Logger(fmt.Sprintf("[TRAJECTORY] Final response length: %d chars (escaped)", len(trajectory.FinalResponse)))
			// Log first 100 chars to verify escaped content is preserved correctly
			preview := trajectory.FinalResponse
			if len(preview) > 100 {
				preview = preview[:100] + "..."
			}
			s.Logger(fmt.Sprintf("[TRAJECTORY] Final response preview (escaped): %s", preview))
		}
	}

	return nil
}

// messagesToMap converts messages to simplified map representation for trajectory
func messagesToMap(msgs []*schema.Message) []map[string]interface{} {
	var result []map[string]interface{}
	for _, msg := range msgs {
		result = append(result, messageToMap(msg))
	}
	return result
}

// escapeForTraining converts content to escaped format for better training visibility
func escapeForTraining(content string) string {
	// Replace actual characters with their escaped representations for training visibility
	content = strings.ReplaceAll(content, "\n", "\\n")
	content = strings.ReplaceAll(content, "\r", "\\r")
	content = strings.ReplaceAll(content, "\t", "\\t")
	content = strings.ReplaceAll(content, "\"", "\\\"")
	content = strings.ReplaceAll(content, "\\", "\\\\")
	return content
}

// messageToMap converts a single message to map with escaped content for training visibility
func messageToMap(msg *schema.Message) map[string]interface{} {
	m := map[string]interface{}{
		"role": string(msg.Role),
	}

	// Escape content for training visibility - show actual escape sequences
	m["content"] = escapeForTraining(msg.Content)

	// Add complete tool calls information if present
	if len(msg.ToolCalls) > 0 {
		var toolCalls []map[string]interface{}
		for _, tc := range msg.ToolCalls {
			toolCall := map[string]interface{}{
				"id":        tc.ID,
				"name":      tc.Function.Name,
				"arguments": escapeForTraining(tc.Function.Arguments),
			}
			toolCalls = append(toolCalls, toolCall)
		}
		m["tool_calls"] = toolCalls
	}

	// Add tool call ID if this is a tool response
	if msg.ToolCallID != "" {
		m["tool_call_id"] = msg.ToolCallID
	}

	// Add tool name if present
	if msg.ToolName != "" {
		m["tool_name"] = msg.ToolName
	}

	return m
}
